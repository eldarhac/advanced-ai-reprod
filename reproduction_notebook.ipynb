{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setting"
      ],
      "metadata": {
        "id": "sGmXbr7F-HTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Drive"
      ],
      "metadata": {
        "id": "YpHAWcYr-LxI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gI3Q0g2dcpv",
        "outputId": "e0234d00-aa97-4ddc-d2d7-f3814a839fc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downgrade Python version to 3.8"
      ],
      "metadata": {
        "id": "R9N4x-Bd-Nsh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc82XmHyJLTz",
        "outputId": "a60f9bdc-1728-46ca-9cd7-109b8eb8548f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\u001b[33m  WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!apt-get update -y > /dev/null\n",
        "!apt-get install python3.8 python3.8-distutils > /dev/null\n",
        "!update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1 > /dev/null\n",
        "!update-alternatives --config python3 > /dev/null\n",
        "!apt-get install python3-pip > /dev/null\n",
        "!python3 -m pip install --upgrade pip --user > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agx2C6NeQdbm",
        "outputId": "b3e2664f-df94-4d39-ff10-eeaa5a8ae88e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version is:  3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0]\n",
            "Printing content of /usr/local/lib/python* to see available versions\n",
            "/usr/local/lib/python3.10:\n",
            "dist-packages\n",
            "\n",
            "/usr/local/lib/python3.8:\n",
            "dist-packages\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "Printing content of /usr/local/lib/python3.11/\n",
            "dist-packages\n"
          ]
        }
      ],
      "source": [
        "#The code below installs 3.11 (assuming you now have 3.10) and restarts environment, so you can run your cells.\n",
        "import sys #for version checker\n",
        "import os #for restart routine\n",
        "\n",
        "if '3.8' in sys.version:\n",
        "  print('You already have 3.8, nothing to install')\n",
        "elif '3.10' in sys.version:\n",
        "  print(\"Python version is: \", sys.version)\n",
        "\n",
        "  print(\"Printing content of /usr/local/lib/python* to see available versions\")\n",
        "  !ls /usr/local/lib/python*\n",
        "\n",
        "  #install python 3.8 and dev utils\n",
        "  #you may not need all the dev libraries, but I haven't tested which aren't necessary.\n",
        "  !sudo apt-get update -y > /dev/null\n",
        "  !sudo apt-get install python3.8 python3.8-dev python3.8-distutils libpython3.8-dev > /dev/null\n",
        "  !sudo apt-get install python3.8-venv binfmt-support  > /dev/null #recommended in install logs of the command above\n",
        "\n",
        "  #change alternatives\n",
        "  !sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1 > /dev/null\n",
        "  !sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 2 > /dev/null\n",
        "\n",
        "  # install pip\n",
        "  !curl -sS https://bootstrap.pypa.io/get-pip.py | python3.8  > /dev/null\n",
        "\n",
        "  #install colab's dependencies\n",
        "  !python3 -m pip install ipython traitlets jupyter psutil matplotlib setuptools ipython_genutils ipykernel jupyter_console notebook prompt_toolkit httplib2 astor  > /dev/null\n",
        "\n",
        "  #minor cleanup\n",
        "  !sudo apt autoremove > /dev/null\n",
        "\n",
        "  #link to the old google package\n",
        "  !ln -s /usr/local/lib/python3.10/dist-packages/google /usr/local/lib/python3.8/dist-packages/google > /dev/null\n",
        "\n",
        "  #this is just to verify if 3.11 folder was indeed created\n",
        "  print(\"Printing content of /usr/local/lib/python3.11/\")\n",
        "  !ls /usr/local/lib/python3.8/\n",
        "\n",
        "  !sed -i \"s/from IPython.utils import traitlets as _traitlets/import traitlets as _traitlets/\" /usr/local/lib/python3.8/dist-packages/google/colab/*.py\n",
        "  !sed -i \"s/from IPython.utils import traitlets/import traitlets/\" /usr/local/lib/python3.8/dist-packages/google/colab/*.py\n",
        "\n",
        "  #restart environment so you don't have to do it manually\n",
        "  os.kill(os.getpid(), 9)\n",
        "else:\n",
        "  print(\"Your out of the box Python is not 3.10, so probably the script will not work, so pls feel free to edit the script to ignore then check and re-run: \", sys.version)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QshxPaxJQ-NX",
        "outputId": "7aeb7f6e-0419-4302-afef-2a9aa97edc5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.8.19 (default, Apr  6 2024, 17:58:10) \n",
            "[GCC 11.4.0]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUuSJmP9zIvS",
        "outputId": "221130d4-afb7-4c8a-da3a-8a79bdb12436"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.8.19\n"
          ]
        }
      ],
      "source": [
        "!python3 --version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Requirements"
      ],
      "metadata": {
        "id": "qI1B2LR1-T7R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqEUPIh9fpR_",
        "outputId": "eabadf93-ea08-40de-8da9-0923afa2e3c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.10.0+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.10.0%2Bcu113-cp38-cp38-linux_x86_64.whl (1821.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.11.0+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.11.0%2Bcu113-cp38-cp38-linux_x86_64.whl (21.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.8/21.8 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.10.0+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.10.0%2Bcu113-cp38-cp38-linux_x86_64.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.10.0+cu113) (4.12.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision==0.11.0+cu113) (1.24.4)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.11.0+cu113) (10.4.0)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "Successfully installed torch-1.10.0+cu113 torchaudio-0.10.0+cu113 torchvision-0.11.0+cu113\n",
            "Collecting transformers==2.11.0\n",
            "  Downloading transformers-2.11.0-py3-none-any.whl.metadata (45 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from transformers==2.11.0) (1.24.4)\n",
            "Collecting tokenizers==0.7.0 (from transformers==2.11.0)\n",
            "  Downloading tokenizers-0.7.0-cp38-cp38-manylinux1_x86_64.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==2.11.0) (24.1)\n",
            "Collecting filelock (from transformers==2.11.0)\n",
            "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==2.11.0) (2.32.3)\n",
            "Collecting tqdm>=4.27 (from transformers==2.11.0)\n",
            "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers==2.11.0)\n",
            "  Downloading regex-2024.7.24-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting sentencepiece (from transformers==2.11.0)\n",
            "  Downloading sentencepiece-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting sacremoses (from transformers==2.11.0)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==2.11.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==2.11.0) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==2.11.0) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==2.11.0) (2024.7.4)\n",
            "Collecting click (from sacremoses->transformers==2.11.0)\n",
            "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting joblib (from sacremoses->transformers==2.11.0)\n",
            "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Downloading transformers-2.11.0-py3-none-any.whl (674 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m674.8/674.8 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.7.0-cp38-cp38-manylinux1_x86_64.whl (7.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m134.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.7.24-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (778 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.9/778.9 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
            "Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentencepiece-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached click-8.1.7-py3-none-any.whl (97 kB)\n",
            "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "Installing collected packages: tokenizers, sentencepiece, tqdm, regex, joblib, filelock, click, sacremoses, transformers\n",
            "Successfully installed click-8.1.7 filelock-3.15.4 joblib-1.4.2 regex-2024.7.24 sacremoses-0.1.1 sentencepiece-0.2.0 tokenizers-0.7.0 tqdm-4.66.5 transformers-2.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.10.0+cu113 torchvision==0.11.0+cu113 torchaudio==0.10.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install transformers==2.11.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqGCj-1TJuko"
      },
      "source": [
        "# Relevant Code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Classes"
      ],
      "metadata": {
        "id": "QXi5X9ah651S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Argument Classes"
      ],
      "metadata": {
        "id": "PHdKco7P6yZd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIzBvAwHRF6l"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    MODEL_WITH_LM_HEAD_MAPPING,\n",
        "    AutoConfig,\n",
        "    AutoModelWithLMHead,\n",
        "    AutoTokenizer,\n",
        "    HfArgumentParser,\n",
        "    PreTrainedTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        "    T5ForConditionalGeneration,\n",
        ")\n",
        "\n",
        "#07/11/2024 14:59:39 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
        "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
        "\n",
        "\n",
        "# Define the argument data classes\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name_or_path: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"The model checkpoint for weights initialization. Leave None if you want to train a model from scratch.\"\n",
        "        },\n",
        "    )\n",
        "    duration_model_path: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"Duration model path\"\n",
        "        },\n",
        "    )\n",
        "    model_type: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n",
        "    )\n",
        "    config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
        "    )\n",
        "    tokenizer_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
        "    )\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
        "    )\n",
        "    eval_model_path: Optional[str] = field(default=None, metadata={\"help\": \"Path to model for evaluation\"})\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    \"\"\"\n",
        "\n",
        "    train_data_file: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The input training data file (a text file).\"}\n",
        "    )\n",
        "    eval_data_file: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n",
        "    )\n",
        "    line_by_line: bool = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\"},\n",
        "    )\n",
        "\n",
        "    mlm: bool = field(\n",
        "        default=False, metadata={\"help\": \"Train with masked-language modeling loss instead of language modeling.\"}\n",
        "    )\n",
        "    mlm_probability: float = field(\n",
        "        default=0.15, metadata={\"help\": \"Ratio of tokens to mask for masked language modeling loss\"}\n",
        "    )\n",
        "\n",
        "    block_size: int = field(\n",
        "        default=-1,\n",
        "        metadata={\n",
        "            \"help\": \"Optional input sequence length after tokenization.\"\n",
        "            \"The training dataset will be truncated in block of this size for training.\"\n",
        "            \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n",
        "        },\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
        "    )\n",
        "\n",
        "@dataclass\n",
        "class TrainingArguments:\n",
        "    output_dir: str = field(default='/content/drive/MyDrive/AI_recovery/ptntime/improved_model/experiment_result')\n",
        "    do_train: bool = field(default=True)\n",
        "    do_eval: bool = field(default=True)\n",
        "    num_train_epochs: int = field(default=50)\n",
        "    per_gpu_train_batch_size: int = field(default=4)\n",
        "    per_device_train_batch_size: int = field(default=4)\n",
        "    gradient_accumulation_steps: int = field(default=4)\n",
        "    per_device_eval_batch_size: int = field(default=4)\n",
        "    per_gpu_eval_batch_size: int = field(default=4)\n",
        "    save_steps: int = field(default=5000)\n",
        "    logging_steps: int = field(default=1000)\n",
        "    overwrite_output_dir: bool = field(default=True)\n",
        "    seed: int = field(default=10)\n",
        "    local_rank=-1\n",
        "    device='cuda'\n",
        "    n_gpu=1\n",
        "    fp16=False\n",
        "    train_batch_size=4\n",
        "    eval_batch_size=4\n",
        "    gradient_accumulation_steps=4\n",
        "    block_size=-1\n",
        "    max_steps=-1\n",
        "    max_grad_norm=1.0\n",
        "    warmup_steps=0\n",
        "    learning_rate=3e-5\n",
        "    weight_decay=0.0\n",
        "    adam_epsilon=1e-8\n",
        "    eval_steps=5000\n",
        "    logging_steps=1000\n",
        "\n",
        "\n",
        "def parse_args_from_dict(params: dict):\n",
        "    model_args = ModelArguments(\n",
        "        model_type=params['model_type'],\n",
        "        tokenizer_name=params['tokenizer_name'],\n",
        "        model_name_or_path=params['model_name_or_path'],\n",
        "        duration_model_path=params['duration_model_path'],\n",
        "        eval_model_path=params['eval_model_path']\n",
        "    )\n",
        "\n",
        "    data_args = DataTrainingArguments(\n",
        "        train_data_file=params['train_data_file'],\n",
        "        eval_data_file=params['eval_data_file'],\n",
        "        line_by_line=params['line_by_line']\n",
        "    )\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=params['output_dir'],\n",
        "        do_train=params['do_train'],\n",
        "        do_eval=params['do_eval'],\n",
        "        num_train_epochs=params['num_train_epochs'],\n",
        "        per_gpu_train_batch_size=params['per_gpu_train_batch_size'],\n",
        "        per_device_train_batch_size=params['per_device_train_batch_size'],\n",
        "        gradient_accumulation_steps=params['gradient_accumulation_steps'],\n",
        "        per_device_eval_batch_size=params['per_device_eval_batch_size'],\n",
        "        per_gpu_eval_batch_size=params['per_gpu_eval_batch_size'],\n",
        "        save_steps=params['save_steps'],\n",
        "        logging_steps=params['logging_steps'],\n",
        "        overwrite_output_dir=params['overwrite_output_dir'],\n",
        "        seed=params['seed']\n",
        "    )\n",
        "\n",
        "    return model_args, data_args, training_args\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIiBZjzWv9mp"
      },
      "source": [
        "### Dataset Object Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsJuPP9DSYmU"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import math\n",
        "import os\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    MODEL_WITH_LM_HEAD_MAPPING,\n",
        "    AutoConfig,\n",
        "    AutoModelWithLMHead,\n",
        "    AutoTokenizer,\n",
        "    HfArgumentParser,\n",
        "    PreTrainedTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        "    T5ForConditionalGeneration,\n",
        ")\n",
        "\n",
        "from transformers.trainer import SequentialDistributedSampler\n",
        "from torch.utils.data.sampler import RandomSampler, Sampler, SequentialSampler\n",
        "\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "\n",
        "class LineByLineTextDataset(Dataset):\n",
        "    \"\"\"\n",
        "    This will be superseded by a framework-agnostic approach\n",
        "    soon.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str):\n",
        "        assert os.path.isfile(file_path)\n",
        "        # Here, we do not cache the features, operating under the assumption\n",
        "        # that we will soon use fast multithreaded tokenizers from the\n",
        "        # `tokenizers` repo everywhere =)\n",
        "        logger.info(\"Creating features from dataset file at %s\", file_path)\n",
        "\n",
        "        with open(file_path, encoding=\"utf-8\") as f:\n",
        "            lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
        "\n",
        "        originals = []\n",
        "        labels = []\n",
        "        for l in lines:\n",
        "            if len(l.split(\"\\t\")) < 2:\n",
        "                continue\n",
        "            originals.append(l.split(\"\\t\")[0])\n",
        "            labels.append(l.split(\"\\t\")[1])\n",
        "\n",
        "        self.inputs = tokenizer.batch_encode_plus(originals, pad_to_max_length=True)\n",
        "        self.labels = tokenizer.batch_encode_plus(labels, pad_to_max_length=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        source_ids = self.inputs[\"input_ids\"][i]\n",
        "        target_ids = self.labels[\"input_ids\"][i]\n",
        "        src_mask = self.inputs[\"attention_mask\"][i]\n",
        "        target_mask = self.labels[\"attention_mask\"][i]\n",
        "        return {\"input_ids\": source_ids, \"attention_mask\": src_mask, \"lm_labels\": target_ids, \"decoder_attention_mask\": target_mask}\n",
        "\n",
        "\n",
        "def get_dataset(args: DataTrainingArguments, tokenizer: PreTrainedTokenizer, evaluate=False):\n",
        "    file_path = args.eval_data_file if evaluate else args.train_data_file\n",
        "    if args.line_by_line:\n",
        "        ret = LineByLineTextDataset(tokenizer=tokenizer, file_path=file_path)\n",
        "        print(\"DATA SIZE: \")\n",
        "        print(len(ret))\n",
        "        return ret\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
        "\n",
        "\n",
        "class LineByLineTextDatasetSymtime(Dataset):\n",
        "    \"\"\"\n",
        "    This will be superseded by a framework-agnostic approach\n",
        "    soon.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str):\n",
        "        assert os.path.isfile(file_path)\n",
        "        # Here, we do not cache the features, operating under the assumption\n",
        "        # that we will soon use fast multithreaded tokenizers from the\n",
        "        # `tokenizers` repo everywhere =)\n",
        "        logger.info(\"Creating features from dataset file at %s\", file_path)\n",
        "\n",
        "        with open(file_path, encoding=\"utf-8\") as f:\n",
        "            lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
        "\n",
        "        inputs_original = []\n",
        "        inputs_start = []\n",
        "        inputs_duration_1 = []\n",
        "        inputs_duration_2 = []\n",
        "\n",
        "        labels_original = []\n",
        "        labels_start = []\n",
        "        labels_duration = []\n",
        "\n",
        "        end_point_labels = []\n",
        "\n",
        "        use_logic_losses = []\n",
        "        use_regular_losses = []\n",
        "\n",
        "        for l in lines:\n",
        "            inputs_original.append(l.split(\"\\t\")[0])\n",
        "            inputs_start.append(l.split(\"\\t\")[1])\n",
        "            inputs_duration_1.append(l.split(\"\\t\")[2])\n",
        "            inputs_duration_2.append(l.split(\"\\t\")[3])\n",
        "\n",
        "            labels_original.append(l.split(\"\\t\")[4])\n",
        "            labels_start.append(\"answer: positive <extra_id_2>\")\n",
        "            labels_duration.append(\"answer: <extra_id_2>\")\n",
        "\n",
        "            epl = 0\n",
        "            if \"ends after\" in l.split(\"\\t\")[0]:\n",
        "                epl = 1\n",
        "            if \"positive\" not in l.split(\"\\t\")[-1]:\n",
        "                epl = -100\n",
        "            end_point_labels.append(epl)\n",
        "\n",
        "            use_logic_loss = 0\n",
        "            use_regular_loss = 1\n",
        "            if \"ends after\" in l.split(\"\\t\")[0] or \"ends before\" in l.split(\"\\t\")[0]:\n",
        "                use_logic_loss = 1\n",
        "                use_regular_loss = 0\n",
        "            use_logic_losses.append(use_logic_loss)\n",
        "            use_regular_losses.append(use_regular_loss)\n",
        "\n",
        "        self.inputs_original = tokenizer.batch_encode_plus(inputs_original, pad_to_max_length=True)\n",
        "        self.inputs_start = tokenizer.batch_encode_plus(inputs_start, pad_to_max_length=True)\n",
        "        self.inputs_duration_1 = tokenizer.batch_encode_plus(inputs_duration_1, pad_to_max_length=True)\n",
        "        self.inputs_duration_2 = tokenizer.batch_encode_plus(inputs_duration_2, pad_to_max_length=True)\n",
        "\n",
        "        self.labels_original = tokenizer.batch_encode_plus(labels_original, pad_to_max_length=True)\n",
        "        self.labels_start = tokenizer.batch_encode_plus(labels_start, pad_to_max_length=True)\n",
        "        self.labels_duration = tokenizer.batch_encode_plus(labels_duration, pad_to_max_length=True)\n",
        "\n",
        "        self.end_point_labels = end_point_labels\n",
        "        self.use_logic_losses = use_logic_losses\n",
        "        self.use_regular_losses = use_regular_losses\n",
        "        assert len(self.use_regular_losses) == len(self.labels_original[\"input_ids\"])\n",
        "        for i, url in enumerate(self.use_regular_losses):\n",
        "            if url == 0:\n",
        "                for j in range(0, 3):\n",
        "                    if self.labels_original[\"input_ids\"][i][j] != 0:\n",
        "                        self.labels_original[\"input_ids\"][i][j] = -100\n",
        "            else:\n",
        "                self.end_point_labels[i] = -100\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs_original[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return {\n",
        "            \"input_ids_original\": self.inputs_original[\"input_ids\"][i],\n",
        "            \"attention_mask_original\": self.inputs_original[\"attention_mask\"][i],\n",
        "            \"input_ids_start\": self.inputs_start[\"input_ids\"][i],\n",
        "            \"attention_mask_start\": self.inputs_start[\"attention_mask\"][i],\n",
        "            \"input_ids_duration_1\": self.inputs_duration_1[\"input_ids\"][i],\n",
        "            \"attention_mask_duration_1\": self.inputs_duration_1[\"attention_mask\"][i],\n",
        "            \"input_ids_duration_2\": self.inputs_duration_2[\"input_ids\"][i],\n",
        "            \"attention_mask_duration_2\": self.inputs_duration_2[\"attention_mask\"][i],\n",
        "            \"lm_labels_original\": self.labels_original[\"input_ids\"][i],\n",
        "            \"lm_labels_start\": self.labels_start[\"input_ids\"][i],\n",
        "            \"lm_labels_duration\": self.labels_duration[\"input_ids\"][i],\n",
        "            \"decoder_attention_mask_original\": [1] * 3 + [0] * (len(self.labels_original[\"attention_mask\"][i]) - 3),\n",
        "            \"decoder_attention_mask_start\": [1] * 4 + [0] * (len(self.labels_start[\"attention_mask\"][i]) - 4),\n",
        "            \"decoder_attention_mask_duration\": [1] * 3 + [0] * (len(self.labels_duration[\"attention_mask\"][i]) - 3),\n",
        "            \"use_logic_loss\": self.use_logic_losses[i],\n",
        "            \"use_regular_loss\": self.use_regular_losses[i],\n",
        "            \"end_point_label\": self.end_point_labels[i],\n",
        "        }\n",
        "\n",
        "\n",
        "def get_dataset_symtime(args: DataTrainingArguments, tokenizer: PreTrainedTokenizer, evaluate=False):\n",
        "    file_path = args.eval_data_file if evaluate else args.train_data_file\n",
        "    if args.line_by_line:\n",
        "        ret = LineByLineTextDatasetSymtime(tokenizer=tokenizer, file_path=file_path)\n",
        "        print(\"DATA SIZE: \")\n",
        "        print(len(ret))\n",
        "        return ret\n",
        "    else:\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAwbz84oxRJR"
      },
      "source": [
        "### T5 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTVaeskYxuqa"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import copy\n",
        "import logging\n",
        "from typing import Any, Dict, List, NewType, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers.data.data_collator import DataCollator\n",
        "from transformers.modeling_t5 import T5PreTrainedModel, T5Stack, T5ForConditionalGeneration\n",
        "\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DoNothingDataCollator(DataCollator):\n",
        "    def collate_batch(self, features) -> Dict[str, torch.Tensor]:\n",
        "        input_ids = torch.tensor([f['input_ids'] for f in features], dtype=torch.long)\n",
        "        attention_mask = torch.tensor([f['attention_mask'] for f in features], dtype=torch.long)\n",
        "        lm_labels = torch.tensor([f['lm_labels'] for f in features], dtype=torch.long)\n",
        "        decoder_attention_mask = torch.tensor([f['decoder_attention_mask'] for f in features], dtype=torch.long)\n",
        "        decoder_inputs = torch.tensor([0, 1525, 10], dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"lm_labels\": lm_labels,\n",
        "            \"decoder_attention_mask\": decoder_attention_mask,\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DoNothingDataCollatorSymtime(DataCollator):\n",
        "    def collate_batch(self, features) -> Dict[str, torch.Tensor]:\n",
        "        input_ids_original = torch.tensor([f['input_ids_original'] for f in features], dtype=torch.long)\n",
        "        input_ids_start = torch.tensor([f['input_ids_start'] for f in features], dtype=torch.long)\n",
        "        input_ids_duration_1 = torch.tensor([f['input_ids_duration_1'] for f in features], dtype=torch.long)\n",
        "        input_ids_duration_2 = torch.tensor([f['input_ids_duration_2'] for f in features], dtype=torch.long)\n",
        "        attention_mask_original = torch.tensor([f['attention_mask_original'] for f in features], dtype=torch.long)\n",
        "        attention_mask_start = torch.tensor([f['attention_mask_start'] for f in features], dtype=torch.long)\n",
        "        attention_mask_duration_1 = torch.tensor([f['attention_mask_duration_1'] for f in features], dtype=torch.long)\n",
        "        attention_mask_duration_2 = torch.tensor([f['attention_mask_duration_2'] for f in features], dtype=torch.long)\n",
        "        lm_labels_original = torch.tensor([f['lm_labels_original'] for f in features], dtype=torch.long)\n",
        "        lm_labels_start = torch.tensor([f['lm_labels_start'] for f in features], dtype=torch.long)\n",
        "        lm_labels_duration = torch.tensor([f['lm_labels_duration'] for f in features], dtype=torch.long)\n",
        "        decoder_attention_mask_original = torch.tensor([f['decoder_attention_mask_original'] for f in features], dtype=torch.long)\n",
        "        decoder_attention_mask_start = torch.tensor([f['decoder_attention_mask_start'] for f in features], dtype=torch.long)\n",
        "        decoder_attention_mask_duration = torch.tensor([f['decoder_attention_mask_duration'] for f in features], dtype=torch.long)\n",
        "        use_logic_loss = torch.tensor([f['use_logic_loss'] for f in features], dtype=torch.long)\n",
        "        use_regular_loss = torch.tensor([f['use_regular_loss'] for f in features], dtype=torch.long)\n",
        "        end_point_label = torch.tensor([f['end_point_label'] for f in features], dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            \"input_ids_original\": input_ids_original,\n",
        "            \"input_ids_start\": input_ids_start,\n",
        "            \"input_ids_duration_1\": input_ids_duration_1,\n",
        "            \"input_ids_duration_2\": input_ids_duration_2,\n",
        "            \"attention_mask_original\": attention_mask_original,\n",
        "            \"attention_mask_start\": attention_mask_start,\n",
        "            \"attention_mask_duration_1\": attention_mask_duration_1,\n",
        "            \"attention_mask_duration_2\": attention_mask_duration_2,\n",
        "            \"lm_labels_original\": lm_labels_original,\n",
        "            \"lm_labels_start\": lm_labels_start,\n",
        "            \"lm_labels_duration\": lm_labels_duration,\n",
        "            \"decoder_attention_mask_original\": decoder_attention_mask_original,\n",
        "            \"decoder_attention_mask_start\": decoder_attention_mask_start,\n",
        "            \"decoder_attention_mask_duration\": decoder_attention_mask_duration,\n",
        "            \"use_logic_loss\": use_logic_loss,\n",
        "            \"use_regular_loss\": use_regular_loss,\n",
        "            \"end_point_label\": end_point_label,\n",
        "        }\n",
        "\n",
        "\n",
        "class T5ForConditionalGenerationCustom(T5PreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.duration_t5_model = T5ForConditionalGeneration(config)\n",
        "        self.model_dim = config.d_model\n",
        "\n",
        "        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n",
        "\n",
        "        encoder_config = copy.deepcopy(config)\n",
        "        self.encoder = T5Stack(encoder_config, self.shared)\n",
        "\n",
        "        decoder_config = copy.deepcopy(config)\n",
        "        decoder_config.is_decoder = True\n",
        "        self.decoder = T5Stack(decoder_config, self.shared)\n",
        "\n",
        "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        self.s = nn.Softmax(dim=1)\n",
        "        self.r = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.init_weights()\n",
        "        self.discrete_value_ids = [32000, 32001, 32002, 32003, 32004, 32005, 32006]\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.shared\n",
        "\n",
        "    def set_input_embeddings(self, new_embeddings):\n",
        "        self.shared = new_embeddings\n",
        "        self.encoder.set_input_embeddings(new_embeddings)\n",
        "        self.decoder.set_input_embeddings(new_embeddings)\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head\n",
        "\n",
        "    def get_encoder(self):\n",
        "        return self.encoder\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.decoder\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids_original=None,\n",
        "        input_ids_start=None,\n",
        "        input_ids_duration_1=None,\n",
        "        input_ids_duration_2=None,\n",
        "        attention_mask_original=None,\n",
        "        attention_mask_start=None,\n",
        "        attention_mask_duration_1=None,\n",
        "        attention_mask_duration_2=None,\n",
        "        decoder_attention_mask_original=None,\n",
        "        decoder_attention_mask_start=None,\n",
        "        decoder_attention_mask_duration=None,\n",
        "        lm_labels_original=None,\n",
        "        lm_labels_start=None,\n",
        "        lm_labels_duration=None,\n",
        "        end_point_label=None,\n",
        "        use_logic_loss=None,\n",
        "        use_regular_loss=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        lm_labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):\n",
        "                Labels for computing the sequence classification/regression loss.\n",
        "                Indices should be in :obj:`[-100, 0, ..., config.vocab_size - 1]`.\n",
        "                All labels set to ``-100`` are ignored (masked), the loss is only\n",
        "                computed for labels in ``[0, ..., config.vocab_size]``\n",
        "\n",
        "    Returns:\n",
        "        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.T5Config`) and inputs.\n",
        "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`lm_label` is provided):\n",
        "            Classification loss (cross entropy).\n",
        "        prediction_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`)\n",
        "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
        "            If `past_key_value_states` is used only the last prediction_scores of the sequences of shape :obj:`(batch_size, 1, hidden_size)` is output.\n",
        "        decoder_past_key_value_states (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`, `optional`, returned when ``use_cache=True``):\n",
        "            Contains pre-computed key and value hidden-states of the attention blocks.\n",
        "            Can be used to speed up sequential decoding (see `decoder_past_key_value_states` input).\n",
        "            Note that when using `decoder_past_key_value_states`, the model only outputs the last `prediction_score` of the sequence of shape :obj:`(batch_size, 1, config.vocab_size)`.\n",
        "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
        "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n",
        "            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention.\n",
        "\n",
        "    Examples::\n",
        "\n",
        "        from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "        tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "        model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "        input_ids = tokenizer.encode(\"Hello, my dog is cute\", return_tensors=\"pt\")  # Batch size 1\n",
        "        outputs = model(input_ids=input_ids, decoder_input_ids=input_ids, lm_labels=input_ids)\n",
        "        loss, prediction_scores = outputs[:2]\n",
        "\n",
        "        tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "        model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "        input_ids = tokenizer.encode(\"summarize: Hello, my dog is cute\", return_tensors=\"pt\")  # Batch size 1\n",
        "        outputs = model.generate(input_ids)\n",
        "        \"\"\"\n",
        "\n",
        "        encoder_outputs_original = self.encoder(\n",
        "            input_ids=input_ids_original, attention_mask=attention_mask_original\n",
        "        )\n",
        "        encoder_outputs_start = self.encoder(\n",
        "            input_ids=input_ids_start, attention_mask=attention_mask_start\n",
        "        )\n",
        "        encoder_outputs_duration_1 = self.duration_t5_model.encoder(\n",
        "            input_ids=input_ids_duration_1, attention_mask=attention_mask_duration_1\n",
        "        )\n",
        "        encoder_outputs_duration_2 = self.duration_t5_model.encoder(\n",
        "            input_ids=input_ids_duration_2, attention_mask=attention_mask_duration_2\n",
        "        )\n",
        "\n",
        "        hidden_states_original = encoder_outputs_original[0]\n",
        "        hidden_states_start = encoder_outputs_start[0]\n",
        "        hidden_states_duration_1 = encoder_outputs_duration_1[0]\n",
        "        hidden_states_duration_2 = encoder_outputs_duration_2[0]\n",
        "\n",
        "        decoder_input_ids_original = self._shift_right(lm_labels_original)\n",
        "        decoder_input_ids_start = self._shift_right(lm_labels_start)\n",
        "        decoder_input_ids_duration = self._shift_right(lm_labels_duration)\n",
        "\n",
        "        decoder_outputs_original = self.decoder(\n",
        "            input_ids=decoder_input_ids_original,\n",
        "            attention_mask=decoder_attention_mask_original,\n",
        "            encoder_hidden_states=hidden_states_original,\n",
        "            encoder_attention_mask=attention_mask_original,\n",
        "            use_cache=False,\n",
        "        )\n",
        "        decoder_outputs_start = self.decoder(\n",
        "            input_ids=decoder_input_ids_start,\n",
        "            attention_mask=decoder_attention_mask_start,\n",
        "            encoder_hidden_states=hidden_states_start,\n",
        "            encoder_attention_mask=attention_mask_start,\n",
        "            use_cache=False,\n",
        "        )\n",
        "\n",
        "        decoder_outputs_duration_1 = self.duration_t5_model.decoder(\n",
        "            input_ids=decoder_input_ids_duration,\n",
        "            attention_mask=decoder_attention_mask_duration,\n",
        "            encoder_hidden_states=hidden_states_duration_1,\n",
        "            encoder_attention_mask=attention_mask_duration_1,\n",
        "            use_cache=False,\n",
        "        )\n",
        "\n",
        "        decoder_outputs_duration_2 = self.duration_t5_model.decoder(\n",
        "            input_ids=decoder_input_ids_duration,\n",
        "            attention_mask=decoder_attention_mask_duration,\n",
        "            encoder_hidden_states=hidden_states_duration_2,\n",
        "            encoder_attention_mask=attention_mask_duration_2,\n",
        "            use_cache=False,\n",
        "        )\n",
        "\n",
        "        sequence_output_original = decoder_outputs_original[0]\n",
        "        sequence_output_original = sequence_output_original * (self.model_dim ** -0.5)\n",
        "        lm_logits_original = self.lm_head(sequence_output_original)\n",
        "\n",
        "        sequence_output_start = decoder_outputs_start[0]\n",
        "        sequence_output_start = sequence_output_start * (self.model_dim ** -0.5)\n",
        "        lm_logits_start = self.lm_head(sequence_output_start)\n",
        "\n",
        "        sequence_output_duration_1 = decoder_outputs_duration_1[0]\n",
        "        sequence_output_duration_1 = sequence_output_duration_1 * (self.model_dim ** -0.5)\n",
        "        lm_logits_duration_1 = self.duration_t5_model.lm_head(sequence_output_duration_1)\n",
        "\n",
        "        sequence_output_duration_2 = decoder_outputs_duration_2[0]\n",
        "        sequence_output_duration_2 = sequence_output_duration_2 * (self.model_dim ** -0.5)\n",
        "        lm_logits_duration_2 = self.duration_t5_model.lm_head(sequence_output_duration_2)\n",
        "\n",
        "        start_before_after_factor = lm_logits_start[:, 2, 1465].view(-1, 1) - lm_logits_start[:, 2, 2841].view(-1, 1)\n",
        "        # Make sure the factor is either -1 or 1.\n",
        "        start_before_after_factor = torch.tanh(start_before_after_factor * 10000.0)\n",
        "\n",
        "        dist = torch.cat((\n",
        "            lm_logits_start[:, 3, self.discrete_value_ids[0]].view(-1, 1),\n",
        "            lm_logits_start[:, 3, self.discrete_value_ids[1]].view(-1, 1),\n",
        "            lm_logits_start[:, 3, self.discrete_value_ids[2]].view(-1, 1),\n",
        "            lm_logits_start[:, 3, self.discrete_value_ids[3]].view(-1, 1),\n",
        "            lm_logits_start[:, 3, self.discrete_value_ids[4]].view(-1, 1),\n",
        "            lm_logits_start[:, 3, self.discrete_value_ids[5]].view(-1, 1),\n",
        "            lm_logits_start[:, 3, self.discrete_value_ids[6]].view(-1, 1),\n",
        "        ), 1)\n",
        "        dist = self.s(dist)\n",
        "\n",
        "        constant_vec = torch.FloatTensor([0, 1, 2, 3, 4, 5, 6]).cuda().repeat(dist.size(0), 1).view(-1, 7, 1)\n",
        "\n",
        "        dist = torch.bmm(dist.view(-1, 1, 7), constant_vec).view(-1, 1) * start_before_after_factor\n",
        "\n",
        "        duration_val_1 = torch.cat((\n",
        "            lm_logits_duration_1[:, 2, self.discrete_value_ids[0]].view(-1, 1),\n",
        "            lm_logits_duration_1[:, 2, self.discrete_value_ids[1]].view(-1, 1),\n",
        "            lm_logits_duration_1[:, 2, self.discrete_value_ids[2]].view(-1, 1),\n",
        "            lm_logits_duration_1[:, 2, self.discrete_value_ids[3]].view(-1, 1),\n",
        "            lm_logits_duration_1[:, 2, self.discrete_value_ids[4]].view(-1, 1),\n",
        "            lm_logits_duration_1[:, 2, self.discrete_value_ids[5]].view(-1, 1),\n",
        "            lm_logits_duration_1[:, 2, self.discrete_value_ids[6]].view(-1, 1),\n",
        "        ), 1)\n",
        "        duration_val_1 = self.s(duration_val_1)\n",
        "\n",
        "        duration_val_1 = torch.bmm(duration_val_1.view(-1, 1, 7), constant_vec).view(-1, 1)\n",
        "\n",
        "        # We use the lm_logits for start time queries\n",
        "        decoder_outputs = (lm_logits_original,) + (lm_logits_start,) + (lm_logits_duration_1,) + (lm_logits_duration_2,)\n",
        "\n",
        "        loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
        "        regular_lm_loss = loss_fct(lm_logits_original.view(-1, lm_logits_original.size(-1)), lm_labels_original.view(-1))\n",
        "\n",
        "        loss = regular_lm_loss\n",
        "\n",
        "        # compute the inference for end time queries\n",
        "        end_logic_loss = -duration_val_1 + dist\n",
        "        decoder_outputs = (end_logic_loss, ) + decoder_outputs\n",
        "        end_logic_loss = torch.cat((end_logic_loss, -end_logic_loss), 1)\n",
        "        end_logic_loss = self.s(end_logic_loss)\n",
        "        end_logic_loss = loss_fct(end_logic_loss, end_point_label.view(-1))\n",
        "\n",
        "        loss += end_logic_loss\n",
        "\n",
        "        decoder_outputs = (loss,) + decoder_outputs\n",
        "\n",
        "        return decoder_outputs\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, past, attention_mask, use_cache, **kwargs):\n",
        "        assert past is not None, \"past has to be defined for encoder_outputs\"\n",
        "\n",
        "        # first step\n",
        "        if len(past) < 2:\n",
        "            encoder_outputs, decoder_past_key_value_states = past, None\n",
        "        else:\n",
        "            encoder_outputs, decoder_past_key_value_states = past[0], past[1]\n",
        "\n",
        "        return {\n",
        "            \"decoder_input_ids\": input_ids,\n",
        "            \"decoder_past_key_value_states\": decoder_past_key_value_states,\n",
        "            \"encoder_outputs\": encoder_outputs,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"use_cache\": use_cache,\n",
        "        }\n",
        "\n",
        "    def _reorder_cache(self, past, beam_idx):\n",
        "        # if decoder past is not included in output\n",
        "        # speedy decoding is disabled and no need to reorder\n",
        "        if len(past) < 2:\n",
        "            return past\n",
        "\n",
        "        decoder_past = past[1]\n",
        "        past = (past[0],)\n",
        "        reordered_decoder_past = ()\n",
        "        for layer_past_states in decoder_past:\n",
        "            # get the correct batch idx from layer past batch dim\n",
        "            # batch dim of `past` is at 2nd position\n",
        "            reordered_layer_past_states = ()\n",
        "            for layer_past_state in layer_past_states:\n",
        "                # need to set correct `past` for each of the four key / value states\n",
        "                reordered_layer_past_states = reordered_layer_past_states + (\n",
        "                    layer_past_state.index_select(0, beam_idx),\n",
        "                )\n",
        "\n",
        "            assert reordered_layer_past_states[0].shape == layer_past_states[0].shape\n",
        "            assert len(reordered_layer_past_states) == len(layer_past_states)\n",
        "\n",
        "            reordered_decoder_past = reordered_decoder_past + (reordered_layer_past_states,)\n",
        "        return past + (reordered_decoder_past,)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training, Evaluation and Test Result Functions"
      ],
      "metadata": {
        "id": "mVnSQsdo6MkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train & Eval Baseline & PTNTIME"
      ],
      "metadata": {
        "id": "CsmfFREK7hz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_eval_base_ptntime(params):\n",
        "    # Parse the arguments\n",
        "    model_args, data_args, training_args = parse_args_from_dict(params)\n",
        "    if data_args.eval_data_file is None and model_args.do_eval:\n",
        "        raise ValueError(\n",
        "            \"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n",
        "            \"or remove the --do_eval argument.\"\n",
        "        )\n",
        "\n",
        "    if (\n",
        "        os.path.exists(training_args.output_dir)\n",
        "        and os.listdir(training_args.output_dir)\n",
        "        and training_args.do_train\n",
        "        and not training_args.overwrite_output_dir\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
        "        )\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
        "    )\n",
        "    logger.warning(\n",
        "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "        training_args.local_rank,\n",
        "        training_args.device,\n",
        "        training_args.n_gpu,\n",
        "        bool(training_args.local_rank != -1),\n",
        "        training_args.fp16,\n",
        "    )\n",
        "    logger.info(\"Training/evaluation parameters %s\", training_args)\n",
        "\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(training_args.seed)\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "\n",
        "    # Distributed training:\n",
        "    # The .from_pretrained methods guarantee that only one local process can concurrently\n",
        "    # download model & vocab.\n",
        "\n",
        "    if model_args.tokenizer_name:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, cache_dir=model_args.cache_dir)\n",
        "    elif model_args.model_name_or_path:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, cache_dir=model_args.cache_dir)\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"You are instantiating a new tokenizer from scratch. This is not supported, but you can do it from another script, save it,\"\n",
        "            \"and load it from here, using --tokenizer_name\"\n",
        "        )\n",
        "\n",
        "    if model_args.model_name_or_path != \"new\":\n",
        "        model = T5ForConditionalGeneration.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "        )\n",
        "    else:\n",
        "        config = AutoConfig.from_pretrained(\"t5-small\")\n",
        "        model = T5ForConditionalGeneration(config=config)\n",
        "\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "\n",
        "    print(data_args.block_size)\n",
        "    if data_args.block_size <= 0:\n",
        "        data_args.block_size = tokenizer.model_max_length\n",
        "        # Our input block size will be the max possible for the model\n",
        "    else:\n",
        "        data_args.block_size = min(data_args.block_size, tokenizer.max_len)\n",
        "\n",
        "    # Get datasets\n",
        "\n",
        "    train_dataset = get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None\n",
        "    eval_dataset = get_dataset(data_args, tokenizer=tokenizer, evaluate=True) if training_args.do_eval else None\n",
        "    data_collator = DoNothingDataCollator()\n",
        "\n",
        "    # Initialize our Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        prediction_loss_only=True,\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    if training_args.do_train:\n",
        "        model_path = (\n",
        "            model_args.model_name_or_path\n",
        "            if model_args.model_name_or_path is not None and os.path.isdir(model_args.model_name_or_path)\n",
        "            else None\n",
        "        )\n",
        "        # trainer.train(model_path=model_path)\n",
        "        trainer.train()\n",
        "        trainer.save_model()\n",
        "        # For convenience, we also re-save the tokenizer to the same directory,\n",
        "        # so that you can share your model easily on huggingface.co/models =)\n",
        "        if trainer.is_world_master():\n",
        "            tokenizer.save_pretrained(training_args.output_dir)\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    if training_args.do_eval:\n",
        "        logger.info(\"*** Evaluate ***\")\n",
        "\n",
        "        model.eval()\n",
        "        sampler = SequentialSampler(eval_dataset)\n",
        "        data_loader = DataLoader(\n",
        "            eval_dataset,\n",
        "            sampler=sampler,\n",
        "            batch_size=training_args.eval_batch_size,\n",
        "            collate_fn=data_collator.collate_batch,\n",
        "        )\n",
        "        output_eval_file = os.path.join(training_args.output_dir, \"eval_results_lm.txt\")\n",
        "        writer = open(output_eval_file, \"w\")\n",
        "        for inputs in tqdm(data_loader, \"Prediction\"):\n",
        "            for k, v in inputs.items():\n",
        "                inputs[k] = v.cuda()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    input_ids=inputs['input_ids'],\n",
        "                    attention_mask=inputs['attention_mask'],\n",
        "                    max_length=12\n",
        "                )\n",
        "                dec = [tokenizer.decode(ids) for ids in outputs]\n",
        "\n",
        "                for i in range(0, len(dec)):\n",
        "                    writer.write(dec[i] + \"\\n\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "8TWna1fd6MXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train & Eval SYMTIME Function"
      ],
      "metadata": {
        "id": "QmwVoz0756lE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_eval_symtime(params):\n",
        "    # See all possible arguments in src/transformers/training_args.py\n",
        "    # or by passing the --help flag to this script.\n",
        "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
        "\n",
        "    model_args, data_args, training_args = parse_args_from_dict(params)\n",
        "\n",
        "    if data_args.eval_data_file is None and training_args.do_eval:\n",
        "        raise ValueError(\n",
        "            \"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n",
        "            \"or remove the --do_eval argument.\"\n",
        "        )\n",
        "\n",
        "    if (\n",
        "        os.path.exists(training_args.output_dir)\n",
        "        and os.listdir(training_args.output_dir)\n",
        "        and training_args.do_train\n",
        "        and not training_args.overwrite_output_dir\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
        "        )\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
        "    )\n",
        "    logger.warning(\n",
        "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "        training_args.local_rank,\n",
        "        training_args.device,\n",
        "        training_args.n_gpu,\n",
        "        bool(training_args.local_rank != -1),\n",
        "        training_args.fp16,\n",
        "    )\n",
        "    logger.info(\"Training/evaluation parameters %s\", training_args)\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(training_args.seed)\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    #\n",
        "    # Distributed training:\n",
        "    # The .from_pretrained methods guarantee that only one local process can concurrently\n",
        "    # download model & vocab.\n",
        "\n",
        "    if model_args.tokenizer_name:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, cache_dir=model_args.cache_dir)\n",
        "    elif model_args.model_name_or_path:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, cache_dir=model_args.cache_dir)\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"You are instantiating a new tokenizer from scratch. This is not supported, but you can do it from another script, save it,\"\n",
        "            \"and load it from here, using --tokenizer_name\"\n",
        "        )\n",
        "\n",
        "    duration_model = T5ForConditionalGeneration.from_pretrained(\n",
        "        model_args.duration_model_path,\n",
        "    )\n",
        "    model = T5ForConditionalGenerationCustom.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "    )\n",
        "    model.duration_t5_model = duration_model\n",
        "\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if data_args.block_size <= 0:\n",
        "        data_args.block_size = tokenizer.max_len\n",
        "        # Our input block size will be the max possible for the model\n",
        "    else:\n",
        "        data_args.block_size = min(data_args.block_size, tokenizer.max_len)\n",
        "\n",
        "    # Get datasets\n",
        "\n",
        "    train_dataset = get_dataset_symtime(data_args, tokenizer=tokenizer) if training_args.do_train else None\n",
        "    eval_dataset = get_dataset_symtime(data_args, tokenizer=tokenizer, evaluate=True) if training_args.do_eval else None\n",
        "    data_collator = DoNothingDataCollatorSymtime()\n",
        "\n",
        "    # Initialize our Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        prediction_loss_only=True,\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    if training_args.do_train:\n",
        "        # trainer.train(model_path=model_path)\n",
        "        trainer.train()\n",
        "        trainer.save_model()\n",
        "        # For convenience, we also re-save the tokenizer to the same directory,\n",
        "        # so that you can share your model easily on huggingface.co/models =)\n",
        "        if trainer.is_world_master():\n",
        "            tokenizer.save_pretrained(training_args.output_dir)\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    if training_args.do_eval:\n",
        "        logger.info(\"*** Evaluate ***\")\n",
        "\n",
        "        # eval_output = trainer.evaluate()\n",
        "        if model_args.eval_model_path != None:\n",
        "            model = T5ForConditionalGenerationCustom.from_pretrained(model_args.eval_model_path).cuda()\n",
        "            model.duration_t5_model = duration_model\n",
        "        else:\n",
        "            model = T5ForConditionalGenerationCustom.from_pretrained(training_args.output_dir).cuda()\n",
        "        model.eval()\n",
        "        sampler = SequentialSampler(eval_dataset)\n",
        "        data_collator = DoNothingDataCollatorSymtime()\n",
        "        data_loader = DataLoader(\n",
        "            eval_dataset,\n",
        "            sampler=sampler,\n",
        "            batch_size=training_args.eval_batch_size,\n",
        "            collate_fn=data_collator.collate_batch,\n",
        "        )\n",
        "        output_eval_file = os.path.join(training_args.output_dir, \"eval_results_lm.txt\")\n",
        "        writer = open(output_eval_file, \"w\")\n",
        "        # 2841 -> negative\n",
        "        # 1465 -> positive\n",
        "        for inputs in tqdm(data_loader, \"Prediction\"):\n",
        "            for k, v in inputs.items():\n",
        "                inputs[k] = v.cuda()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs_lm_logits = model(**inputs)[2].detach().cpu().numpy()\n",
        "                outputs_end = model(**inputs)[1].detach().cpu().numpy()\n",
        "\n",
        "                for klm in range(0, len(outputs_lm_logits)):\n",
        "                    label_1 = \"positive\"\n",
        "                    if outputs_lm_logits[klm][2][2841] > outputs_lm_logits[klm][2][1465]:\n",
        "                        label_1 = \"negative\"\n",
        "                    label_2 = str(outputs_end[klm])\n",
        "                    writer.write(\"\\t\".join([label_1, label_2]) + \"\\n\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "cXgUgjOC563k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### General Training & Evaluation Function"
      ],
      "metadata": {
        "id": "MFh9oJQY50mx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZo8YOVXKK5R"
      },
      "outputs": [],
      "source": [
        "from transformers.data.data_collator import DataCollator\n",
        "from typing import Dict\n",
        "\n",
        "def run_and_eval(model_name_or_path: str, output_dir: str, train_data_file: str,\n",
        "                 eval_data_file: str, per_gpu_train_batch_size: int = 4,\n",
        "                 per_device_train_batch_size: int = 4, gradient_accumulation_steps: int = 4,\n",
        "                 per_device_eval_batch_size: int = 4, per_gpu_eval_batch_size: int = 4,\n",
        "                 save_steps: int = 10000, duration_model_path: str = None,\n",
        "                 do_train=True, eval_model_path=None):\n",
        "    params = {\n",
        "      \"model_type\": \"t5\",\n",
        "      \"tokenizer_name\": \"t5-large\",\n",
        "      \"model_name_or_path\": model_name_or_path,\n",
        "      \"duration_model_path\": duration_model_path,\n",
        "      \"output_dir\": output_dir,\n",
        "      \"do_train\": do_train,\n",
        "      \"do_eval\": True,\n",
        "      \"num_train_epochs\": 50,\n",
        "      \"train_data_file\": train_data_file,\n",
        "      \"eval_data_file\": eval_data_file,\n",
        "      \"line_by_line\": True,\n",
        "      \"per_gpu_train_batch_size\": per_gpu_train_batch_size,\n",
        "      \"per_device_train_batch_size\": per_device_train_batch_size,\n",
        "      \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "      \"per_device_eval_batch_size\": per_device_eval_batch_size,\n",
        "      \"per_gpu_eval_batch_size\": per_gpu_eval_batch_size,\n",
        "      \"save_steps\": save_steps,\n",
        "      \"logging_steps\": 100,\n",
        "      \"overwrite_output_dir\": True,\n",
        "      \"seed\": 10,\n",
        "      \"eval_model_path\": eval_model_path\n",
        "    }\n",
        "\n",
        "    if 'symtime' in model_name_or_path:\n",
        "        return train_and_eval_symtime(params)\n",
        "    else:\n",
        "        return train_and_eval_base_ptntime(params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Results - Baseline & PTNTIME"
      ],
      "metadata": {
        "id": "DMb--NLv5zyZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoelTDeUmrOO"
      },
      "outputs": [],
      "source": [
        "def evaluate_tracie_style(test_path: str, result_path: str, end:bool=True):\n",
        "    glines = [x.strip() for x in open(test_path).readlines()]\n",
        "    plines = [x.strip() for x in open(f\"{result_path}/eval_results_lm.txt\").readlines()]\n",
        "    assert len(glines) == len(plines)\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    total_start = 0\n",
        "    correct_start = 0\n",
        "    total_end = 0\n",
        "    correct_end = 0\n",
        "    story_prediction_map = {}\n",
        "    for i, l in enumerate(glines):\n",
        "        if \"story:\" in l.split(\"\\t\")[0]:\n",
        "            story = l.split(\"\\t\")[0].split(\"story:\")[1]\n",
        "        else:\n",
        "            story = \"no story\"\n",
        "        if story not in story_prediction_map:\n",
        "            story_prediction_map[story] = []\n",
        "        label = l.split(\"\\t\")[1].split()[1]\n",
        "        p = plines[i].split()[1][:8]\n",
        "        total += 1\n",
        "        if label == p:\n",
        "            correct += 1\n",
        "            story_prediction_map[story].append(True)\n",
        "        else:\n",
        "            story_prediction_map[story].append(False)\n",
        "        if \"starts before\" in l or \"starts after\" in l:\n",
        "            total_start += 1\n",
        "            if label == p:\n",
        "                correct_start += 1\n",
        "        else:\n",
        "            total_end += 1\n",
        "            if label == p:\n",
        "                correct_end += 1\n",
        "    s_total = 0\n",
        "    s_correct = 0\n",
        "    for key in story_prediction_map:\n",
        "        s_total += 1\n",
        "        cv = True\n",
        "        for v in story_prediction_map[key]:\n",
        "            cv = cv and v\n",
        "        if cv:\n",
        "            s_correct += 1\n",
        "    print(\"Overall Acc: {}\".format(str(float(correct) / float(total))))\n",
        "    print(\"Start Acc: {}\".format(str(float(correct_start) / float(total_start))))\n",
        "    if end:\n",
        "      print(\"End Acc: {}\".format(str(float(correct_end) / float(total_end))))\n",
        "    print(\"Story Acc: {}\".format(str(float(s_correct) / float(s_total))))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Results - SYMTIME"
      ],
      "metadata": {
        "id": "6AHy3PCn5zLY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-JuW4vkbpwI"
      },
      "outputs": [],
      "source": [
        "def evaluate_symbolic(test_path: str, result_path: str, end:bool=True):\n",
        "    glines = [x.strip() for x in open(test_path).readlines()]\n",
        "    plines = [x.strip() for x in open(f\"{result_path}/eval_results_lm.txt\").readlines()]\n",
        "    assert len(glines) == len(plines)\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    total_start = 0\n",
        "    correct_start = 0\n",
        "    total_end = 0\n",
        "    correct_end = 0\n",
        "    story_prediction_map = {}\n",
        "    for i, l in enumerate(glines):\n",
        "        story = l.split(\"\\t\")[0].split(\"story:\")[1]\n",
        "        if story not in story_prediction_map:\n",
        "            story_prediction_map[story] = []\n",
        "        label = l.split(\"\\t\")[-1].split()[1]\n",
        "        p = plines[i].split(\"\\t\")\n",
        "        if \"starts before\" in l.split(\"\\t\")[0] or \"starts after\" in l.split(\"\\t\")[0]:\n",
        "            total_start += 1\n",
        "            total += 1\n",
        "            if label == p[0]:\n",
        "                correct_start += 1\n",
        "                correct += 1\n",
        "                story_prediction_map[story].append(True)\n",
        "            else:\n",
        "                story_prediction_map[story].append(False)\n",
        "        else:\n",
        "            p = float(p[1][1:-1])\n",
        "            pl = \"before\"\n",
        "            if p < 0:\n",
        "                pl = \"after\"\n",
        "            if label != \"positive\":\n",
        "                continue\n",
        "            total_end += 1\n",
        "            total += 1\n",
        "            label = \"before\"\n",
        "            if \"ends after\" in l.split(\"\\t\")[0]:\n",
        "                label = \"after\"\n",
        "            if label == pl:\n",
        "                correct_end += 1\n",
        "                correct += 1\n",
        "                story_prediction_map[story].append(True)\n",
        "            else:\n",
        "                story_prediction_map[story].append(False)\n",
        "\n",
        "    # print(\"Overall Acc: {}\".format(str(float(correct) / float(total))))\n",
        "    print(\"Start Acc: {}\".format(str(float(correct_start) / float(total_start))))\n",
        "    if end:\n",
        "      print(\"End Acc: {}\".format(str(float(correct_end) / float(total_end))))\n",
        "    print(\"Overall Acc: {}\".format(str(float(correct) / float(total))))\n",
        "\n",
        "    s_total = 0\n",
        "    s_correct = 0\n",
        "    for key in story_prediction_map:\n",
        "        s_total += 1\n",
        "        cv = True\n",
        "        for v in story_prediction_map[key]:\n",
        "            cv = cv and v\n",
        "        if cv:\n",
        "            s_correct += 1\n",
        "    print(\"Story Acc: {}\".format(str(float(s_correct) / float(s_total))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Process Prerequisites"
      ],
      "metadata": {
        "id": "oBeu8wDK8V_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate MATRES Minimal Supervision Train set"
      ],
      "metadata": {
        "id": "uhyMyYK2fkhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "DATA_DIR = '/content/drive/MyDrive/tracie'\n",
        "MATRES_TRAIN_DATA_FILE = os.path.join(DATA_DIR, 'data/matres/matres_train_before_after_tracie_style.txt')\n",
        "MATRES_MINIMAL_SUPERVISION_TRAIN_DATA_FILE = os.path.join(DATA_DIR, 'data/matres/matres_minimal_supervision_train.txt')\n",
        "\n",
        "\n",
        "# randomly choose 6% of rows in MATRES_TRAIN_DATA_FILE and save it to MATRES_MINIMAL_SUPERVISION_TRAIN_DATA_FILE\n",
        "with open(MATRES_TRAIN_DATA_FILE, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    random.shuffle(lines)\n",
        "    with open(MATRES_MINIMAL_SUPERVISION_TRAIN_DATA_FILE, 'w') as f2:\n",
        "        for line in lines[:int(len(lines) * 0.06)]:\n",
        "            f2.write(line)"
      ],
      "metadata": {
        "id": "szwARIMCfkGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configurations"
      ],
      "metadata": {
        "id": "xmUh0knL5yYH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRZK46K3n7Xr"
      },
      "outputs": [],
      "source": [
        "MODELS_DIR = '/content/drive/MyDrive/TRACIE_MODELS'\n",
        "DATA_DIR = '/content/drive/MyDrive/tracie'\n",
        "\n",
        "SYMTIME_PRETRAINED_MODEL_DURATION = os.path.join(MODELS_DIR, 'symtime-pretrained-model/duration')\n",
        "SYMTIME_PRETRAINED_MODEL_START = os.path.join(MODELS_DIR, 'symtime-pretrained-model/start')\n",
        "PTNTIME_PRETRAINED_MODEL = os.path.join(MODELS_DIR, 'ptntime-pretrained-model')\n",
        "\n",
        "IID_TRAIN_DATA_FILE = os.path.join(DATA_DIR, 'data/iid/tracie_train.txt')\n",
        "IID_EVAL_DATA_FILE = os.path.join(DATA_DIR, 'data/iid/tracie_test.txt')\n",
        "\n",
        "UNIFORM_TRAIN_DATA_FILE = os.path.join(DATA_DIR, 'data/uniform-prior/tracie_train_uniform_prior.txt')\n",
        "UNIFORM_EVAL_DATA_FILE = os.path.join(DATA_DIR, 'data/uniform-prior/tracie_test.txt')\n",
        "\n",
        "IID_TRAIN_DATA_FILE_SYMBOLIC = os.path.join(DATA_DIR, 'data/iid-symbolic-format/train.txt')\n",
        "IID_EVAL_DATA_FILE_SYMBOLIC = os.path.join(DATA_DIR, 'data/iid-symbolic-format/test.txt')\n",
        "\n",
        "UNIFORM_TRAIN_DATA_FILE_SYMBOLIC = os.path.join(DATA_DIR, 'data/uniform-prior-symbolic-format/train.txt')\n",
        "UNIFORM_EVAL_DATA_FILE_SYMBOLIC = os.path.join(DATA_DIR, 'data/uniform-prior-symbolic-format/test.txt')\n",
        "\n",
        "MATRES_MINIMAL_SUPERVISION_TRAIN_DATA_FILE = os.path.join(DATA_DIR, 'data/matres/matres_minimal_supervision_train.txt')\n",
        "MATRES_TRAIN_DATA_FILE = os.path.join(DATA_DIR, 'data/matres/matres_train_before_after_tracie_style.txt')\n",
        "MATRES_EVAL_DATA_FILE = os.path.join(DATA_DIR, 'data/matres/matres_test_before_after_tracie_style.txt')\n",
        "\n",
        "config_dict = {\n",
        "    'iid': {\n",
        "        'base_model': {'model_name_or_path': 't5-large',\n",
        "                       'output_dir': os.path.join(DATA_DIR, 'experiments/iid/base'),\n",
        "                       'train_data_file': IID_TRAIN_DATA_FILE, 'eval_data_file': IID_EVAL_DATA_FILE},\n",
        "        'ptntime': {'model_name_or_path': PTNTIME_PRETRAINED_MODEL,\n",
        "                    'output_dir': os.path.join(DATA_DIR, 'experiments/iid/ptntime'),\n",
        "                    'train_data_file': IID_TRAIN_DATA_FILE, 'eval_data_file': IID_EVAL_DATA_FILE},\n",
        "        'symtime': {'model_name_or_path': SYMTIME_PRETRAINED_MODEL_START,\n",
        "                    'duration_model_path': SYMTIME_PRETRAINED_MODEL_DURATION,\n",
        "                    'output_dir': os.path.join(DATA_DIR, 'experiments/iid/symtime'),\n",
        "                    'train_data_file': IID_TRAIN_DATA_FILE_SYMBOLIC,\n",
        "                    'eval_data_file': IID_EVAL_DATA_FILE_SYMBOLIC}\n",
        "    },\n",
        "    'uniform': {\n",
        "        'base_model': {'model_name_or_path': 't5-large',\n",
        "                       'output_dir': os.path.join(DATA_DIR, 'experiments/uniform/base'),\n",
        "                       'train_data_file': UNIFORM_TRAIN_DATA_FILE, 'eval_data_file': UNIFORM_EVAL_DATA_FILE},\n",
        "        'ptntime': {'model_name_or_path': PTNTIME_PRETRAINED_MODEL,\n",
        "                    'output_dir': os.path.join(DATA_DIR, 'experiments/uniform/ptntime'),\n",
        "                    'train_data_file': UNIFORM_TRAIN_DATA_FILE, 'eval_data_file': UNIFORM_EVAL_DATA_FILE},\n",
        "        'symtime': {'model_name_or_path': SYMTIME_PRETRAINED_MODEL_START,\n",
        "                    'duration_model_path': SYMTIME_PRETRAINED_MODEL_DURATION,\n",
        "                    'output_dir': os.path.join(DATA_DIR, 'experiments/uniform/symtime'),\n",
        "                    'train_data_file': UNIFORM_TRAIN_DATA_FILE_SYMBOLIC, 'eval_data_file': UNIFORM_EVAL_DATA_FILE_SYMBOLIC}\n",
        "    },\n",
        "    'matres': {\n",
        "        'base_model': {'model_name_or_path': 't5-large',\n",
        "                       'output_dir': os.path.join(DATA_DIR, 'experiments/matres/base'),\n",
        "                       'train_data_file': MATRES_MINIMAL_SUPERVISION_TRAIN_DATA_FILE, 'eval_data_file': MATRES_EVAL_DATA_FILE},\n",
        "        'ptntime': {'model_name_or_path': PTNTIME_PRETRAINED_MODEL,\n",
        "                    'output_dir': os.path.join(DATA_DIR, 'experiments/matres/ptntime'),\n",
        "                    'train_data_file': MATRES_MINIMAL_SUPERVISION_TRAIN_DATA_FILE, 'eval_data_file': MATRES_EVAL_DATA_FILE}\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training, Evaluation & Results"
      ],
      "metadata": {
        "id": "FabZV50v8k-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tracie - iid"
      ],
      "metadata": {
        "id": "66Ph8rU722Jr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Baseline (T5)**"
      ],
      "metadata": {
        "id": "aAIXT-1H87On"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_and_eval(**config_dict['iid']['base_model'])\n",
        "print('evaluating iid base model:')\n",
        "evaluate_tracie_style(config_dict['iid']['base_model']['eval_data_file'], config_dict['iid']['base_model']['output_dir'])"
      ],
      "metadata": {
        "id": "2ZAp2uE91sS8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8227aa34-12b6-4c55-fb30-76471596d0a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "DATA SIZE: \n",
            "1174\n",
            "DATA SIZE: \n",
            "4248\n",
            "WARNING:transformers.trainer:You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.\n",
            "WARNING:transformers.training_args:Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "WARNING:transformers.training_args:Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "{\"loss\": 0.5674817403052294, \"learning_rate\": 4.931972789115647e-05, \"epoch\": 0.6802721088435374, \"step\": 100}\n",
            "{\"loss\": 0.3894329728274715, \"learning_rate\": 4.8639455782312926e-05, \"epoch\": 1.3605442176870748, \"step\": 200}\n",
            "{\"loss\": 0.37037132921915655, \"learning_rate\": 4.795918367346939e-05, \"epoch\": 2.0408163265306123, \"step\": 300}\n",
            "{\"loss\": 0.3440597821139883, \"learning_rate\": 4.7278911564625856e-05, \"epoch\": 2.7210884353741496, \"step\": 400}\n",
            "{\"loss\": 0.33177959885204017, \"learning_rate\": 4.6598639455782315e-05, \"epoch\": 3.4013605442176873, \"step\": 500}\n",
            "{\"loss\": 0.3048851138791946, \"learning_rate\": 4.591836734693878e-05, \"epoch\": 4.081632653061225, \"step\": 600}\n",
            "{\"loss\": 0.31922931993293474, \"learning_rate\": 4.523809523809524e-05, \"epoch\": 4.761904761904762, \"step\": 700}\n",
            "{\"loss\": 0.28449142818232304, \"learning_rate\": 4.4557823129251704e-05, \"epoch\": 5.442176870748299, \"step\": 800}\n",
            "{\"loss\": 0.3011797657654012, \"learning_rate\": 4.387755102040816e-05, \"epoch\": 6.122448979591836, \"step\": 900}\n",
            "{\"loss\": 0.29964813077343533, \"learning_rate\": 4.319727891156463e-05, \"epoch\": 6.802721088435375, \"step\": 1000}\n",
            "{\"loss\": 0.29589882165305537, \"learning_rate\": 4.2517006802721085e-05, \"epoch\": 7.482993197278912, \"step\": 1100}\n",
            "{\"loss\": 0.3076696118664205, \"learning_rate\": 4.183673469387756e-05, \"epoch\": 8.16326530612245, \"step\": 1200}\n",
            "{\"loss\": 0.2976186270867208, \"learning_rate\": 4.1156462585034016e-05, \"epoch\": 8.843537414965986, \"step\": 1300}\n",
            "{\"loss\": 0.27271089649443925, \"learning_rate\": 4.047619047619048e-05, \"epoch\": 9.523809523809524, \"step\": 1400}\n",
            "{\"loss\": 0.2754356770752895, \"learning_rate\": 3.979591836734694e-05, \"epoch\": 10.204081632653061, \"step\": 1500}\n",
            "{\"loss\": 0.2642052024398231, \"learning_rate\": 3.9115646258503405e-05, \"epoch\": 10.884353741496598, \"step\": 1600}\n",
            "{\"loss\": 0.2994986714301308, \"learning_rate\": 3.843537414965986e-05, \"epoch\": 11.564625850340136, \"step\": 1700}\n",
            "{\"loss\": 0.2655152613396967, \"learning_rate\": 3.775510204081633e-05, \"epoch\": 12.244897959183673, \"step\": 1800}\n",
            "{\"loss\": 0.285722593545222, \"learning_rate\": 3.707482993197279e-05, \"epoch\": 12.92517006802721, \"step\": 1900}\n",
            "{\"loss\": 0.29809478512618853, \"learning_rate\": 3.639455782312925e-05, \"epoch\": 13.60544217687075, \"step\": 2000}\n",
            "{\"loss\": 0.2601583390543874, \"learning_rate\": 3.571428571428572e-05, \"epoch\": 14.285714285714286, \"step\": 2100}\n",
            "{\"loss\": 0.2805265924437697, \"learning_rate\": 3.5034013605442175e-05, \"epoch\": 14.965986394557824, \"step\": 2200}\n",
            "{\"loss\": 0.3054684591381408, \"learning_rate\": 3.435374149659864e-05, \"epoch\": 15.646258503401361, \"step\": 2300}\n",
            "{\"loss\": 0.2620829265939483, \"learning_rate\": 3.36734693877551e-05, \"epoch\": 16.3265306122449, \"step\": 2400}\n",
            "{\"loss\": 0.2765101975861398, \"learning_rate\": 3.2993197278911564e-05, \"epoch\": 17.006802721088434, \"step\": 2500}\n",
            "{\"loss\": 0.2879646143538855, \"learning_rate\": 3.231292517006803e-05, \"epoch\": 17.687074829931973, \"step\": 2600}\n",
            "{\"loss\": 0.2615643983974155, \"learning_rate\": 3.1632653061224494e-05, \"epoch\": 18.367346938775512, \"step\": 2700}\n",
            "{\"loss\": 0.26587439749391817, \"learning_rate\": 3.095238095238095e-05, \"epoch\": 19.047619047619047, \"step\": 2800}\n",
            "{\"loss\": 0.27784762251457096, \"learning_rate\": 3.0272108843537418e-05, \"epoch\": 19.727891156462587, \"step\": 2900}\n",
            "{\"loss\": 0.2923298768325481, \"learning_rate\": 2.959183673469388e-05, \"epoch\": 20.408163265306122, \"step\": 3000}\n",
            "{\"loss\": 0.2949659788165536, \"learning_rate\": 2.891156462585034e-05, \"epoch\": 21.08843537414966, \"step\": 3100}\n",
            "{\"loss\": 0.2801391840558585, \"learning_rate\": 2.8231292517006803e-05, \"epoch\": 21.768707482993197, \"step\": 3200}\n",
            "{\"loss\": 0.2522974204936304, \"learning_rate\": 2.7551020408163265e-05, \"epoch\": 22.448979591836736, \"step\": 3300}\n",
            "{\"loss\": 0.2699763020538444, \"learning_rate\": 2.687074829931973e-05, \"epoch\": 23.12925170068027, \"step\": 3400}\n",
            "{\"loss\": 0.2864313523824831, \"learning_rate\": 2.6190476190476192e-05, \"epoch\": 23.80952380952381, \"step\": 3500}\n",
            "{\"loss\": 0.28884068224200293, \"learning_rate\": 2.5510204081632654e-05, \"epoch\": 24.489795918367346, \"step\": 3600}\n",
            "{\"loss\": 0.30239897694300455, \"learning_rate\": 2.4829931972789116e-05, \"epoch\": 25.170068027210885, \"step\": 3700}\n",
            "{\"loss\": 0.26343029590815603, \"learning_rate\": 2.4149659863945578e-05, \"epoch\": 25.85034013605442, \"step\": 3800}\n",
            "{\"loss\": 0.2851321961935446, \"learning_rate\": 2.3469387755102043e-05, \"epoch\": 26.53061224489796, \"step\": 3900}\n",
            "{\"loss\": 0.27930179099144425, \"learning_rate\": 2.2789115646258505e-05, \"epoch\": 27.2108843537415, \"step\": 4000}\n",
            "{\"loss\": 0.2696461662621391, \"learning_rate\": 2.2108843537414966e-05, \"epoch\": 27.891156462585034, \"step\": 4100}\n",
            "{\"loss\": 0.24993839773462467, \"learning_rate\": 2.1428571428571428e-05, \"epoch\": 28.571428571428573, \"step\": 4200}\n",
            "{\"loss\": 0.2757426105648346, \"learning_rate\": 2.0748299319727893e-05, \"epoch\": 29.25170068027211, \"step\": 4300}\n",
            "{\"loss\": 0.2553730021669935, \"learning_rate\": 2.0068027210884355e-05, \"epoch\": 29.931972789115648, \"step\": 4400}\n",
            "{\"loss\": 0.2670703710254861, \"learning_rate\": 1.9387755102040817e-05, \"epoch\": 30.612244897959183, \"step\": 4500}\n",
            "{\"loss\": 0.2591904162775336, \"learning_rate\": 1.8707482993197282e-05, \"epoch\": 31.292517006802722, \"step\": 4600}\n",
            "{\"loss\": 0.2843211040346614, \"learning_rate\": 1.8027210884353744e-05, \"epoch\": 31.972789115646258, \"step\": 4700}\n",
            "{\"loss\": 0.2681089238022537, \"learning_rate\": 1.7346938775510206e-05, \"epoch\": 32.6530612244898, \"step\": 4800}\n",
            "{\"loss\": 0.2684918992896814, \"learning_rate\": 1.6666666666666667e-05, \"epoch\": 33.333333333333336, \"step\": 4900}\n",
            "{\"loss\": 0.27647141468666175, \"learning_rate\": 1.5986394557823133e-05, \"epoch\": 34.01360544217687, \"step\": 5000}\n",
            "{\"loss\": 0.23773559453364668, \"learning_rate\": 1.5306122448979594e-05, \"epoch\": 34.69387755102041, \"step\": 5100}\n",
            "{\"loss\": 0.2732532965373662, \"learning_rate\": 1.4625850340136055e-05, \"epoch\": 35.374149659863946, \"step\": 5200}\n",
            "{\"loss\": 0.24920634144315726, \"learning_rate\": 1.3945578231292516e-05, \"epoch\": 36.054421768707485, \"step\": 5300}\n",
            "{\"loss\": 0.243638031303542, \"learning_rate\": 1.3265306122448982e-05, \"epoch\": 36.734693877551024, \"step\": 5400}\n",
            "{\"loss\": 0.24290363445480578, \"learning_rate\": 1.2585034013605443e-05, \"epoch\": 37.414965986394556, \"step\": 5500}\n",
            "{\"loss\": 0.26243784703165374, \"learning_rate\": 1.1904761904761905e-05, \"epoch\": 38.095238095238095, \"step\": 5600}\n",
            "{\"loss\": 0.25880438433765673, \"learning_rate\": 1.1224489795918369e-05, \"epoch\": 38.775510204081634, \"step\": 5700}\n",
            "{\"loss\": 0.23847089663418275, \"learning_rate\": 1.054421768707483e-05, \"epoch\": 39.45578231292517, \"step\": 5800}\n",
            "{\"loss\": 0.2784806598063392, \"learning_rate\": 9.863945578231292e-06, \"epoch\": 40.136054421768705, \"step\": 5900}\n",
            "{\"loss\": 0.2446115591955072, \"learning_rate\": 9.183673469387756e-06, \"epoch\": 40.816326530612244, \"step\": 6000}\n",
            "{\"loss\": 0.2723523109819416, \"learning_rate\": 8.503401360544217e-06, \"epoch\": 41.49659863945578, \"step\": 6100}\n",
            "{\"loss\": 0.23081659441888178, \"learning_rate\": 7.823129251700681e-06, \"epoch\": 42.17687074829932, \"step\": 6200}\n",
            "{\"loss\": 0.2891345555491512, \"learning_rate\": 7.142857142857143e-06, \"epoch\": 42.857142857142854, \"step\": 6300}\n",
            "{\"loss\": 0.23781042947708556, \"learning_rate\": 6.462585034013606e-06, \"epoch\": 43.53741496598639, \"step\": 6400}\n",
            "{\"loss\": 0.25281025819533853, \"learning_rate\": 5.782312925170069e-06, \"epoch\": 44.21768707482993, \"step\": 6500}\n",
            "{\"loss\": 0.25270445937626845, \"learning_rate\": 5.102040816326531e-06, \"epoch\": 44.89795918367347, \"step\": 6600}\n",
            "{\"loss\": 0.2543973135171382, \"learning_rate\": 4.421768707482993e-06, \"epoch\": 45.578231292517, \"step\": 6700}\n",
            "{\"loss\": 0.25803586699897324, \"learning_rate\": 3.741496598639456e-06, \"epoch\": 46.25850340136054, \"step\": 6800}\n",
            "{\"loss\": 0.2667428669916944, \"learning_rate\": 3.0612244897959185e-06, \"epoch\": 46.93877551020408, \"step\": 6900}\n",
            "{\"loss\": 0.2327250903783738, \"learning_rate\": 2.3809523809523808e-06, \"epoch\": 47.61904761904762, \"step\": 7000}\n",
            "{\"loss\": 0.2475751905797938, \"learning_rate\": 1.7006802721088438e-06, \"epoch\": 48.29931972789116, \"step\": 7100}\n",
            "{\"loss\": 0.2651885964003122, \"learning_rate\": 1.020408163265306e-06, \"epoch\": 48.97959183673469, \"step\": 7200}\n",
            "{\"loss\": 0.28029308538254327, \"learning_rate\": 3.4013605442176873e-07, \"epoch\": 49.65986394557823, \"step\": 7300}\n",
            "WARNING:transformers.training_args:Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "WARNING:__main__:Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "\n",
            "evaluating iid base model:\n",
            "Overall Acc: 0.739406779661017\n",
            "Start Acc: 0.7219334719334719\n",
            "End Acc: 0.7538726333907056\n",
            "Story Acc: 0.23391812865497075\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **PTNTIME**"
      ],
      "metadata": {
        "id": "Buyie-zn3b8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_and_eval(**config_dict['iid']['ptntime'])\n",
        "print('evaluating iid ptntime:')\n",
        "evaluate_tracie_style(config_dict['iid']['ptntime']['eval_data_file'], config_dict['iid']['ptntime']['output_dir'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzFL4EzC3atj",
        "outputId": "331d0bc3-d226-4078-8213-a1e8c2360e64"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "DATA SIZE: \n",
            "1174\n",
            "DATA SIZE: \n",
            "4248\n",
            "WARNING:transformers.trainer:You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.\n",
            "WARNING:transformers.training_args:Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "WARNING:transformers.training_args:Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "{\"loss\": 0.5674817403052294, \"learning_rate\": 4.931972789115647e-05, \"epoch\": 0.6802721088435374, \"step\": 100}\n",
            "{\"loss\": 0.3894329728274715, \"learning_rate\": 4.8639455782312926e-05, \"epoch\": 1.3605442176870748, \"step\": 200}\n",
            "{\"loss\": 0.37037132921915655, \"learning_rate\": 4.795918367346939e-05, \"epoch\": 2.0408163265306123, \"step\": 300}\n",
            "{\"loss\": 0.3440597821139883, \"learning_rate\": 4.7278911564625856e-05, \"epoch\": 2.7210884353741496, \"step\": 400}\n",
            "{\"loss\": 0.33177959885204017, \"learning_rate\": 4.6598639455782315e-05, \"epoch\": 3.4013605442176873, \"step\": 500}\n",
            "{\"loss\": 0.3048851138791946, \"learning_rate\": 4.591836734693878e-05, \"epoch\": 4.081632653061225, \"step\": 600}\n",
            "{\"loss\": 0.31922931993293474, \"learning_rate\": 4.523809523809524e-05, \"epoch\": 4.761904761904762, \"step\": 700}\n",
            "{\"loss\": 0.28449142818232304, \"learning_rate\": 4.4557823129251704e-05, \"epoch\": 5.442176870748299, \"step\": 800}\n",
            "{\"loss\": 0.3011797657654012, \"learning_rate\": 4.387755102040816e-05, \"epoch\": 6.122448979591836, \"step\": 900}\n",
            "{\"loss\": 0.29964813077343533, \"learning_rate\": 4.319727891156463e-05, \"epoch\": 6.802721088435375, \"step\": 1000}\n",
            "{\"loss\": 0.29589882165305537, \"learning_rate\": 4.2517006802721085e-05, \"epoch\": 7.482993197278912, \"step\": 1100}\n",
            "{\"loss\": 0.3076696118664205, \"learning_rate\": 4.183673469387756e-05, \"epoch\": 8.16326530612245, \"step\": 1200}\n",
            "{\"loss\": 0.2976186270867208, \"learning_rate\": 4.1156462585034016e-05, \"epoch\": 8.843537414965986, \"step\": 1300}\n",
            "{\"loss\": 0.27271089649443925, \"learning_rate\": 4.047619047619048e-05, \"epoch\": 9.523809523809524, \"step\": 1400}\n",
            "{\"loss\": 0.2754356770752895, \"learning_rate\": 3.979591836734694e-05, \"epoch\": 10.204081632653061, \"step\": 1500}\n",
            "{\"loss\": 0.2642052024398231, \"learning_rate\": 3.9115646258503405e-05, \"epoch\": 10.884353741496598, \"step\": 1600}\n",
            "{\"loss\": 0.2994986714301308, \"learning_rate\": 3.843537414965986e-05, \"epoch\": 11.564625850340136, \"step\": 1700}\n",
            "{\"loss\": 0.2655152613396967, \"learning_rate\": 3.775510204081633e-05, \"epoch\": 12.244897959183673, \"step\": 1800}\n",
            "{\"loss\": 0.285722593545222, \"learning_rate\": 3.707482993197279e-05, \"epoch\": 12.92517006802721, \"step\": 1900}\n",
            "{\"loss\": 0.29809478512618853, \"learning_rate\": 3.639455782312925e-05, \"epoch\": 13.60544217687075, \"step\": 2000}\n",
            "{\"loss\": 0.2601583390543874, \"learning_rate\": 3.571428571428572e-05, \"epoch\": 14.285714285714286, \"step\": 2100}\n",
            "{\"loss\": 0.2805265924437697, \"learning_rate\": 3.5034013605442175e-05, \"epoch\": 14.965986394557824, \"step\": 2200}\n",
            "{\"loss\": 0.3054684591381408, \"learning_rate\": 3.435374149659864e-05, \"epoch\": 15.646258503401361, \"step\": 2300}\n",
            "{\"loss\": 0.2620829265939483, \"learning_rate\": 3.36734693877551e-05, \"epoch\": 16.3265306122449, \"step\": 2400}\n",
            "{\"loss\": 0.2765101975861398, \"learning_rate\": 3.2993197278911564e-05, \"epoch\": 17.006802721088434, \"step\": 2500}\n",
            "{\"loss\": 0.2879646143538855, \"learning_rate\": 3.231292517006803e-05, \"epoch\": 17.687074829931973, \"step\": 2600}\n",
            "{\"loss\": 0.2615643983974155, \"learning_rate\": 3.1632653061224494e-05, \"epoch\": 18.367346938775512, \"step\": 2700}\n",
            "{\"loss\": 0.26587439749391817, \"learning_rate\": 3.095238095238095e-05, \"epoch\": 19.047619047619047, \"step\": 2800}\n",
            "{\"loss\": 0.27784762251457096, \"learning_rate\": 3.0272108843537418e-05, \"epoch\": 19.727891156462587, \"step\": 2900}\n",
            "{\"loss\": 0.2923298768325481, \"learning_rate\": 2.959183673469388e-05, \"epoch\": 20.408163265306122, \"step\": 3000}\n",
            "{\"loss\": 0.2949659788165536, \"learning_rate\": 2.891156462585034e-05, \"epoch\": 21.08843537414966, \"step\": 3100}\n",
            "{\"loss\": 0.2801391840558585, \"learning_rate\": 2.8231292517006803e-05, \"epoch\": 21.768707482993197, \"step\": 3200}\n",
            "{\"loss\": 0.2522974204936304, \"learning_rate\": 2.7551020408163265e-05, \"epoch\": 22.448979591836736, \"step\": 3300}\n",
            "{\"loss\": 0.2699763020538444, \"learning_rate\": 2.687074829931973e-05, \"epoch\": 23.12925170068027, \"step\": 3400}\n",
            "{\"loss\": 0.2864313523824831, \"learning_rate\": 2.6190476190476192e-05, \"epoch\": 23.80952380952381, \"step\": 3500}\n",
            "{\"loss\": 0.28884068224200293, \"learning_rate\": 2.5510204081632654e-05, \"epoch\": 24.489795918367346, \"step\": 3600}\n",
            "{\"loss\": 0.30239897694300455, \"learning_rate\": 2.4829931972789116e-05, \"epoch\": 25.170068027210885, \"step\": 3700}\n",
            "{\"loss\": 0.26343029590815603, \"learning_rate\": 2.4149659863945578e-05, \"epoch\": 25.85034013605442, \"step\": 3800}\n",
            "{\"loss\": 0.2851321961935446, \"learning_rate\": 2.3469387755102043e-05, \"epoch\": 26.53061224489796, \"step\": 3900}\n",
            "{\"loss\": 0.27930179099144425, \"learning_rate\": 2.2789115646258505e-05, \"epoch\": 27.2108843537415, \"step\": 4000}\n",
            "{\"loss\": 0.2696461662621391, \"learning_rate\": 2.2108843537414966e-05, \"epoch\": 27.891156462585034, \"step\": 4100}\n",
            "{\"loss\": 0.24993839773462467, \"learning_rate\": 2.1428571428571428e-05, \"epoch\": 28.571428571428573, \"step\": 4200}\n",
            "{\"loss\": 0.2757426105648346, \"learning_rate\": 2.0748299319727893e-05, \"epoch\": 29.25170068027211, \"step\": 4300}\n",
            "{\"loss\": 0.2553730021669935, \"learning_rate\": 2.0068027210884355e-05, \"epoch\": 29.931972789115648, \"step\": 4400}\n",
            "{\"loss\": 0.2670703710254861, \"learning_rate\": 1.9387755102040817e-05, \"epoch\": 30.612244897959183, \"step\": 4500}\n",
            "{\"loss\": 0.2591904162775336, \"learning_rate\": 1.8707482993197282e-05, \"epoch\": 31.292517006802722, \"step\": 4600}\n",
            "{\"loss\": 0.2843211040346614, \"learning_rate\": 1.8027210884353744e-05, \"epoch\": 31.972789115646258, \"step\": 4700}\n",
            "{\"loss\": 0.2681089238022537, \"learning_rate\": 1.7346938775510206e-05, \"epoch\": 32.6530612244898, \"step\": 4800}\n",
            "{\"loss\": 0.2684918992896814, \"learning_rate\": 1.6666666666666667e-05, \"epoch\": 33.333333333333336, \"step\": 4900}\n",
            "{\"loss\": 0.27647141468666175, \"learning_rate\": 1.5986394557823133e-05, \"epoch\": 34.01360544217687, \"step\": 5000}\n",
            "{\"loss\": 0.23773559453364668, \"learning_rate\": 1.5306122448979594e-05, \"epoch\": 34.69387755102041, \"step\": 5100}\n",
            "{\"loss\": 0.2732532965373662, \"learning_rate\": 1.4625850340136055e-05, \"epoch\": 35.374149659863946, \"step\": 5200}\n",
            "{\"loss\": 0.24920634144315726, \"learning_rate\": 1.3945578231292516e-05, \"epoch\": 36.054421768707485, \"step\": 5300}\n",
            "{\"loss\": 0.243638031303542, \"learning_rate\": 1.3265306122448982e-05, \"epoch\": 36.734693877551024, \"step\": 5400}\n",
            "{\"loss\": 0.24290363445480578, \"learning_rate\": 1.2585034013605443e-05, \"epoch\": 37.414965986394556, \"step\": 5500}\n",
            "{\"loss\": 0.26243784703165374, \"learning_rate\": 1.1904761904761905e-05, \"epoch\": 38.095238095238095, \"step\": 5600}\n",
            "{\"loss\": 0.25880438433765673, \"learning_rate\": 1.1224489795918369e-05, \"epoch\": 38.775510204081634, \"step\": 5700}\n",
            "{\"loss\": 0.23847089663418275, \"learning_rate\": 1.054421768707483e-05, \"epoch\": 39.45578231292517, \"step\": 5800}\n",
            "{\"loss\": 0.2784806598063392, \"learning_rate\": 9.863945578231292e-06, \"epoch\": 40.136054421768705, \"step\": 5900}\n",
            "{\"loss\": 0.2446115591955072, \"learning_rate\": 9.183673469387756e-06, \"epoch\": 40.816326530612244, \"step\": 6000}\n",
            "{\"loss\": 0.2723523109819416, \"learning_rate\": 8.503401360544217e-06, \"epoch\": 41.49659863945578, \"step\": 6100}\n",
            "{\"loss\": 0.23081659441888178, \"learning_rate\": 7.823129251700681e-06, \"epoch\": 42.17687074829932, \"step\": 6200}\n",
            "{\"loss\": 0.2891345555491512, \"learning_rate\": 7.142857142857143e-06, \"epoch\": 42.857142857142854, \"step\": 6300}\n",
            "{\"loss\": 0.23781042947708556, \"learning_rate\": 6.462585034013606e-06, \"epoch\": 43.53741496598639, \"step\": 6400}\n",
            "{\"loss\": 0.25281025819533853, \"learning_rate\": 5.782312925170069e-06, \"epoch\": 44.21768707482993, \"step\": 6500}\n",
            "{\"loss\": 0.25270445937626845, \"learning_rate\": 5.102040816326531e-06, \"epoch\": 44.89795918367347, \"step\": 6600}\n",
            "{\"loss\": 0.2543973135171382, \"learning_rate\": 4.421768707482993e-06, \"epoch\": 45.578231292517, \"step\": 6700}\n",
            "{\"loss\": 0.25803586699897324, \"learning_rate\": 3.741496598639456e-06, \"epoch\": 46.25850340136054, \"step\": 6800}\n",
            "{\"loss\": 0.2667428669916944, \"learning_rate\": 3.0612244897959185e-06, \"epoch\": 46.93877551020408, \"step\": 6900}\n",
            "{\"loss\": 0.2327250903783738, \"learning_rate\": 2.3809523809523808e-06, \"epoch\": 47.61904761904762, \"step\": 7000}\n",
            "{\"loss\": 0.2475751905797938, \"learning_rate\": 1.7006802721088438e-06, \"epoch\": 48.29931972789116, \"step\": 7100}\n",
            "{\"loss\": 0.2651885964003122, \"learning_rate\": 1.020408163265306e-06, \"epoch\": 48.97959183673469, \"step\": 7200}\n",
            "{\"loss\": 0.28029308538254327, \"learning_rate\": 3.4013605442176873e-07, \"epoch\": 49.65986394557823, \"step\": 7300}\n",
            "WARNING:transformers.training_args:Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "WARNING:__main__:Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "\n",
            "evaluating iid ptntime:\n",
            "Overall Acc: 0.8020244821092278\n",
            "Start Acc: 0.814968814968815\n",
            "End Acc: 0.7913080895008606\n",
            "Story Acc: 0.3157894736842105\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **SYMTIME**"
      ],
      "metadata": {
        "id": "G2ZOi94e33Da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_and_eval(**config_dict['iid']['symtime'], per_gpu_train_batch_size=4,\n",
        "                 per_device_train_batch_size=4, gradient_accumulation_steps=2,\n",
        "                 per_device_eval_batch_size=4, per_gpu_eval_batch_size=4)\n",
        "print('evaluating iid symtime:')\n",
        "evaluate_symbolic(config_dict['iid']['symtime']['eval_data_file'], config_dict['iid']['symtime']['output_dir'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lb5G0Hth33Z0",
        "outputId": "df512cd9-1ce9-4a50-aadf-9e140867d22f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "DATA SIZE: \n",
            "1174\n",
            "DATA SIZE: \n",
            "4248\n",
            "WARNING:transformers.trainer:You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.\n",
            "WARNING:transformers.training_args:Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "WARNING:transformers.training_args:Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "{\"loss\": 0.5674817403052294, \"learning_rate\": 4.931972789115647e-05, \"epoch\": 0.6802721088435374, \"step\": 100}\n",
            "{\"loss\": 0.3894329728274715, \"learning_rate\": 4.8639455782312926e-05, \"epoch\": 1.3605442176870748, \"step\": 200}\n",
            "{\"loss\": 0.37037132921915655, \"learning_rate\": 4.795918367346939e-05, \"epoch\": 2.0408163265306123, \"step\": 300}\n",
            "{\"loss\": 0.3440597821139883, \"learning_rate\": 4.7278911564625856e-05, \"epoch\": 2.7210884353741496, \"step\": 400}\n",
            "{\"loss\": 0.33177959885204017, \"learning_rate\": 4.6598639455782315e-05, \"epoch\": 3.4013605442176873, \"step\": 500}\n",
            "{\"loss\": 0.3048851138791946, \"learning_rate\": 4.591836734693878e-05, \"epoch\": 4.081632653061225, \"step\": 600}\n",
            "{\"loss\": 0.31922931993293474, \"learning_rate\": 4.523809523809524e-05, \"epoch\": 4.761904761904762, \"step\": 700}\n",
            "{\"loss\": 0.28449142818232304, \"learning_rate\": 4.4557823129251704e-05, \"epoch\": 5.442176870748299, \"step\": 800}\n",
            "{\"loss\": 0.3011797657654012, \"learning_rate\": 4.387755102040816e-05, \"epoch\": 6.122448979591836, \"step\": 900}\n",
            "{\"loss\": 0.29964813077343533, \"learning_rate\": 4.319727891156463e-05, \"epoch\": 6.802721088435375, \"step\": 1000}\n",
            "{\"loss\": 0.29589882165305537, \"learning_rate\": 4.2517006802721085e-05, \"epoch\": 7.482993197278912, \"step\": 1100}\n",
            "{\"loss\": 0.3076696118664205, \"learning_rate\": 4.183673469387756e-05, \"epoch\": 8.16326530612245, \"step\": 1200}\n",
            "{\"loss\": 0.2976186270867208, \"learning_rate\": 4.1156462585034016e-05, \"epoch\": 8.843537414965986, \"step\": 1300}\n",
            "{\"loss\": 0.27271089649443925, \"learning_rate\": 4.047619047619048e-05, \"epoch\": 9.523809523809524, \"step\": 1400}\n",
            "{\"loss\": 0.2754356770752895, \"learning_rate\": 3.979591836734694e-05, \"epoch\": 10.204081632653061, \"step\": 1500}\n",
            "{\"loss\": 0.2642052024398231, \"learning_rate\": 3.9115646258503405e-05, \"epoch\": 10.884353741496598, \"step\": 1600}\n",
            "{\"loss\": 0.2994986714301308, \"learning_rate\": 3.843537414965986e-05, \"epoch\": 11.564625850340136, \"step\": 1700}\n",
            "{\"loss\": 0.2655152613396967, \"learning_rate\": 3.775510204081633e-05, \"epoch\": 12.244897959183673, \"step\": 1800}\n",
            "{\"loss\": 0.285722593545222, \"learning_rate\": 3.707482993197279e-05, \"epoch\": 12.92517006802721, \"step\": 1900}\n",
            "{\"loss\": 0.29809478512618853, \"learning_rate\": 3.639455782312925e-05, \"epoch\": 13.60544217687075, \"step\": 2000}\n",
            "{\"loss\": 0.2601583390543874, \"learning_rate\": 3.571428571428572e-05, \"epoch\": 14.285714285714286, \"step\": 2100}\n",
            "{\"loss\": 0.2805265924437697, \"learning_rate\": 3.5034013605442175e-05, \"epoch\": 14.965986394557824, \"step\": 2200}\n",
            "{\"loss\": 0.3054684591381408, \"learning_rate\": 3.435374149659864e-05, \"epoch\": 15.646258503401361, \"step\": 2300}\n",
            "{\"loss\": 0.2620829265939483, \"learning_rate\": 3.36734693877551e-05, \"epoch\": 16.3265306122449, \"step\": 2400}\n",
            "{\"loss\": 0.2765101975861398, \"learning_rate\": 3.2993197278911564e-05, \"epoch\": 17.006802721088434, \"step\": 2500}\n",
            "{\"loss\": 0.2879646143538855, \"learning_rate\": 3.231292517006803e-05, \"epoch\": 17.687074829931973, \"step\": 2600}\n",
            "{\"loss\": 0.2615643983974155, \"learning_rate\": 3.1632653061224494e-05, \"epoch\": 18.367346938775512, \"step\": 2700}\n",
            "{\"loss\": 0.26587439749391817, \"learning_rate\": 3.095238095238095e-05, \"epoch\": 19.047619047619047, \"step\": 2800}\n",
            "{\"loss\": 0.27784762251457096, \"learning_rate\": 3.0272108843537418e-05, \"epoch\": 19.727891156462587, \"step\": 2900}\n",
            "{\"loss\": 0.2923298768325481, \"learning_rate\": 2.959183673469388e-05, \"epoch\": 20.408163265306122, \"step\": 3000}\n",
            "{\"loss\": 0.2949659788165536, \"learning_rate\": 2.891156462585034e-05, \"epoch\": 21.08843537414966, \"step\": 3100}\n",
            "{\"loss\": 0.2801391840558585, \"learning_rate\": 2.8231292517006803e-05, \"epoch\": 21.768707482993197, \"step\": 3200}\n",
            "{\"loss\": 0.2522974204936304, \"learning_rate\": 2.7551020408163265e-05, \"epoch\": 22.448979591836736, \"step\": 3300}\n",
            "{\"loss\": 0.2699763020538444, \"learning_rate\": 2.687074829931973e-05, \"epoch\": 23.12925170068027, \"step\": 3400}\n",
            "{\"loss\": 0.2864313523824831, \"learning_rate\": 2.6190476190476192e-05, \"epoch\": 23.80952380952381, \"step\": 3500}\n",
            "{\"loss\": 0.28884068224200293, \"learning_rate\": 2.5510204081632654e-05, \"epoch\": 24.489795918367346, \"step\": 3600}\n",
            "{\"loss\": 0.30239897694300455, \"learning_rate\": 2.4829931972789116e-05, \"epoch\": 25.170068027210885, \"step\": 3700}\n",
            "{\"loss\": 0.26343029590815603, \"learning_rate\": 2.4149659863945578e-05, \"epoch\": 25.85034013605442, \"step\": 3800}\n",
            "{\"loss\": 0.2851321961935446, \"learning_rate\": 2.3469387755102043e-05, \"epoch\": 26.53061224489796, \"step\": 3900}\n",
            "{\"loss\": 0.27930179099144425, \"learning_rate\": 2.2789115646258505e-05, \"epoch\": 27.2108843537415, \"step\": 4000}\n",
            "{\"loss\": 0.2696461662621391, \"learning_rate\": 2.2108843537414966e-05, \"epoch\": 27.891156462585034, \"step\": 4100}\n",
            "{\"loss\": 0.24993839773462467, \"learning_rate\": 2.1428571428571428e-05, \"epoch\": 28.571428571428573, \"step\": 4200}\n",
            "{\"loss\": 0.2757426105648346, \"learning_rate\": 2.0748299319727893e-05, \"epoch\": 29.25170068027211, \"step\": 4300}\n",
            "{\"loss\": 0.2553730021669935, \"learning_rate\": 2.0068027210884355e-05, \"epoch\": 29.931972789115648, \"step\": 4400}\n",
            "{\"loss\": 0.2670703710254861, \"learning_rate\": 1.9387755102040817e-05, \"epoch\": 30.612244897959183, \"step\": 4500}\n",
            "{\"loss\": 0.2591904162775336, \"learning_rate\": 1.8707482993197282e-05, \"epoch\": 31.292517006802722, \"step\": 4600}\n",
            "{\"loss\": 0.2843211040346614, \"learning_rate\": 1.8027210884353744e-05, \"epoch\": 31.972789115646258, \"step\": 4700}\n",
            "{\"loss\": 0.2681089238022537, \"learning_rate\": 1.7346938775510206e-05, \"epoch\": 32.6530612244898, \"step\": 4800}\n",
            "{\"loss\": 0.2684918992896814, \"learning_rate\": 1.6666666666666667e-05, \"epoch\": 33.333333333333336, \"step\": 4900}\n",
            "{\"loss\": 0.27647141468666175, \"learning_rate\": 1.5986394557823133e-05, \"epoch\": 34.01360544217687, \"step\": 5000}\n",
            "{\"loss\": 0.23773559453364668, \"learning_rate\": 1.5306122448979594e-05, \"epoch\": 34.69387755102041, \"step\": 5100}\n",
            "{\"loss\": 0.2732532965373662, \"learning_rate\": 1.4625850340136055e-05, \"epoch\": 35.374149659863946, \"step\": 5200}\n",
            "{\"loss\": 0.24920634144315726, \"learning_rate\": 1.3945578231292516e-05, \"epoch\": 36.054421768707485, \"step\": 5300}\n",
            "{\"loss\": 0.243638031303542, \"learning_rate\": 1.3265306122448982e-05, \"epoch\": 36.734693877551024, \"step\": 5400}\n",
            "{\"loss\": 0.24290363445480578, \"learning_rate\": 1.2585034013605443e-05, \"epoch\": 37.414965986394556, \"step\": 5500}\n",
            "{\"loss\": 0.26243784703165374, \"learning_rate\": 1.1904761904761905e-05, \"epoch\": 38.095238095238095, \"step\": 5600}\n",
            "{\"loss\": 0.25880438433765673, \"learning_rate\": 1.1224489795918369e-05, \"epoch\": 38.775510204081634, \"step\": 5700}\n",
            "{\"loss\": 0.23847089663418275, \"learning_rate\": 1.054421768707483e-05, \"epoch\": 39.45578231292517, \"step\": 5800}\n",
            "{\"loss\": 0.2784806598063392, \"learning_rate\": 9.863945578231292e-06, \"epoch\": 40.136054421768705, \"step\": 5900}\n",
            "{\"loss\": 0.2446115591955072, \"learning_rate\": 9.183673469387756e-06, \"epoch\": 40.816326530612244, \"step\": 6000}\n",
            "{\"loss\": 0.2723523109819416, \"learning_rate\": 8.503401360544217e-06, \"epoch\": 41.49659863945578, \"step\": 6100}\n",
            "{\"loss\": 0.23081659441888178, \"learning_rate\": 7.823129251700681e-06, \"epoch\": 42.17687074829932, \"step\": 6200}\n",
            "{\"loss\": 0.2891345555491512, \"learning_rate\": 7.142857142857143e-06, \"epoch\": 42.857142857142854, \"step\": 6300}\n",
            "{\"loss\": 0.23781042947708556, \"learning_rate\": 6.462585034013606e-06, \"epoch\": 43.53741496598639, \"step\": 6400}\n",
            "{\"loss\": 0.25281025819533853, \"learning_rate\": 5.782312925170069e-06, \"epoch\": 44.21768707482993, \"step\": 6500}\n",
            "{\"loss\": 0.25270445937626845, \"learning_rate\": 5.102040816326531e-06, \"epoch\": 44.89795918367347, \"step\": 6600}\n",
            "{\"loss\": 0.2543973135171382, \"learning_rate\": 4.421768707482993e-06, \"epoch\": 45.578231292517, \"step\": 6700}\n",
            "{\"loss\": 0.25803586699897324, \"learning_rate\": 3.741496598639456e-06, \"epoch\": 46.25850340136054, \"step\": 6800}\n",
            "{\"loss\": 0.2667428669916944, \"learning_rate\": 3.0612244897959185e-06, \"epoch\": 46.93877551020408, \"step\": 6900}\n",
            "{\"loss\": 0.2327250903783738, \"learning_rate\": 2.3809523809523808e-06, \"epoch\": 47.61904761904762, \"step\": 7000}\n",
            "{\"loss\": 0.2475751905797938, \"learning_rate\": 1.7006802721088438e-06, \"epoch\": 48.29931972789116, \"step\": 7100}\n",
            "{\"loss\": 0.2651885964003122, \"learning_rate\": 1.020408163265306e-06, \"epoch\": 48.97959183673469, \"step\": 7200}\n",
            "{\"loss\": 0.28029308538254327, \"learning_rate\": 3.4013605442176873e-07, \"epoch\": 49.65986394557823, \"step\": 7300}\n",
            "WARNING:transformers.training_args:Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "WARNING:__main__:Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "\n",
            "evaluating iid symtime:\n",
            "Start Acc: 0.8118503118503119\n",
            "End Acc: 0.7891566265060241\n",
            "Overall Acc: 0.8033052495139339\n",
            "Story Acc: 0.32456140350877194\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **SYMTIME-Zero Shot**"
      ],
      "metadata": {
        "id": "BSD2GNPq45QM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_and_eval(**config_dict['iid']['symtime'], per_gpu_train_batch_size=4,\n",
        "                 per_device_train_batch_size=4, gradient_accumulation_steps=2,\n",
        "                 per_device_eval_batch_size=4, per_gpu_eval_batch_size=4,\n",
        "                 do_train=False, eval_model_path=SYMTIME_PRETRAINED_MODEL_START)\n",
        "print('evaluating iid symtime zero-shot:')\n",
        "evaluate_symbolic(config_dict['iid']['symtime']['eval_data_file'], config_dict['iid']['symtime']['output_dir'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlwE8Eo745hW",
        "outputId": "431aadf2-6dad-4539-f951-df7ffbfaa879"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "DATA SIZE: \n",
            "4248\n",
            "WARNING:transformers.trainer:You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.\n",
            "WARNING:transformers.training_args:Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "WARNING:__main__:Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "evaluating iid symtime zero-shot:\n",
            "Start Acc: 0.7697505197505198\n",
            "End Acc: 0.7306368330464716\n",
            "Overall Acc: 0.7550226830848995\n",
            "Story Acc: 0.21637426900584794\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tracie - Uniform Prior"
      ],
      "metadata": {
        "id": "bgJFlH569JLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Baseline (T5)**"
      ],
      "metadata": {
        "id": "JraFt-Jl4JLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_and_eval(**config_dict['uniform']['base_model'])\n",
        "print('evaluating uniform base model:')\n",
        "evaluate_tracie_style(config_dict['uniform']['base_model']['eval_data_file'], config_dict['uniform']['base_model']['output_dir'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "269gWjBq4Jcf",
        "outputId": "bd8f2737-aeff-44d2-a21d-8bf4d7e586ec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "-1\n",
            "DATA SIZE: \n",
            "860\n",
            "DATA SIZE: \n",
            "4248\n",
            "WARNING:transformers.trainer:You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.\n",
            "WARNING:transformers.training_args:Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "WARNING:transformers.training_args:Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "{\"loss\": 1.0533595700934528, \"learning_rate\": 4.811320754716982e-05, \"epoch\": 1.8744186046511628, \"step\": 100}\n",
            "{\"loss\": 0.2591566316783428, \"learning_rate\": 4.6226415094339625e-05, \"epoch\": 3.7627906976744185, \"step\": 200}\n",
            "{\"loss\": 0.2512530894204974, \"learning_rate\": 4.433962264150944e-05, \"epoch\": 5.651162790697675, \"step\": 300}\n",
            "{\"loss\": 0.2492764936387539, \"learning_rate\": 4.245283018867925e-05, \"epoch\": 7.53953488372093, \"step\": 400}\n",
            "{\"loss\": 0.24679180853068827, \"learning_rate\": 4.0566037735849064e-05, \"epoch\": 9.427906976744186, \"step\": 500}\n",
            "{\"loss\": 0.24502138283103705, \"learning_rate\": 3.867924528301887e-05, \"epoch\": 11.316279069767441, \"step\": 600}\n",
            "{\"loss\": 0.238896558098495, \"learning_rate\": 3.679245283018868e-05, \"epoch\": 13.204651162790698, \"step\": 700}\n",
            "{\"loss\": 0.24377585470676422, \"learning_rate\": 3.490566037735849e-05, \"epoch\": 15.093023255813954, \"step\": 800}\n",
            "{\"loss\": 0.23991581056267022, \"learning_rate\": 3.30188679245283e-05, \"epoch\": 16.967441860465115, \"step\": 900}\n",
            "{\"loss\": 0.2416757708787918, \"learning_rate\": 3.113207547169811e-05, \"epoch\": 18.855813953488372, \"step\": 1000}\n",
            "{\"loss\": 0.22313445942476393, \"learning_rate\": 2.9245283018867926e-05, \"epoch\": 20.74418604651163, \"step\": 1100}\n",
            "{\"loss\": 0.19164862680248917, \"learning_rate\": 2.7358490566037738e-05, \"epoch\": 22.632558139534883, \"step\": 1200}\n",
            "{\"loss\": 0.1540495018020738, \"learning_rate\": 2.547169811320755e-05, \"epoch\": 24.52093023255814, \"step\": 1300}\n",
            "{\"loss\": 0.08705849158083766, \"learning_rate\": 2.358490566037736e-05, \"epoch\": 26.409302325581397, \"step\": 1400}\n",
            "{\"loss\": 0.06045042765494145, \"learning_rate\": 2.1698113207547172e-05, \"epoch\": 28.29767441860465, \"step\": 1500}\n",
            "{\"loss\": 0.04498441955106955, \"learning_rate\": 1.9811320754716984e-05, \"epoch\": 30.186046511627907, \"step\": 1600}\n",
            "{\"loss\": 0.024355510232971937, \"learning_rate\": 1.7924528301886792e-05, \"epoch\": 32.074418604651164, \"step\": 1700}\n",
            "{\"loss\": 0.018391763505782137, \"learning_rate\": 1.6037735849056604e-05, \"epoch\": 33.948837209302326, \"step\": 1800}\n",
            "{\"loss\": 0.013923481884472153, \"learning_rate\": 1.4150943396226415e-05, \"epoch\": 35.83720930232558, \"step\": 1900}\n",
            "{\"loss\": 0.010664360948559305, \"learning_rate\": 1.2264150943396227e-05, \"epoch\": 37.72558139534884, \"step\": 2000}\n",
            "{\"loss\": 0.011041490384845929, \"learning_rate\": 1.0377358490566038e-05, \"epoch\": 39.61395348837209, \"step\": 2100}\n",
            "{\"loss\": 0.008636457411683977, \"learning_rate\": 8.49056603773585e-06, \"epoch\": 41.50232558139535, \"step\": 2200}\n",
            "{\"loss\": 0.007063043822568602, \"learning_rate\": 6.60377358490566e-06, \"epoch\": 43.390697674418604, \"step\": 2300}\n",
            "{\"loss\": 0.007817261071557481, \"learning_rate\": 4.716981132075472e-06, \"epoch\": 45.27906976744186, \"step\": 2400}\n",
            "{\"loss\": 0.010196408969238178, \"learning_rate\": 2.830188679245283e-06, \"epoch\": 47.16744186046512, \"step\": 2500}\n",
            "{\"loss\": 0.007695655017698187, \"learning_rate\": 9.433962264150943e-07, \"epoch\": 49.055813953488375, \"step\": 2600}\n",
            "WARNING:transformers.training_args:Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "WARNING:__main__:Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "evaluating uniform base model:\n",
            "Overall Acc: 0.6800847457627118\n",
            "Start Acc: 0.6658004158004158\n",
            "End Acc: 0.6919104991394148\n",
            "Story Acc: 0.15789473684210525\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **PTNTIME**"
      ],
      "metadata": {
        "id": "O7AukwIU4dmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_and_eval(**config_dict['uniform']['ptntime'])\n",
        "print('evaluating uniform ptntime:')\n",
        "evaluate_tracie_style(config_dict['uniform']['ptntime']['eval_data_file'], config_dict['uniform']['ptntime']['output_dir'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGsWZOGT4d66",
        "outputId": "2e1fab28-b624-44e8-8d5a-401d70f2d6c7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1\n",
            "DATA SIZE: \n",
            "860\n",
            "DATA SIZE: \n",
            "4248\n",
            "WARNING:transformers.trainer:You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.\n",
            "WARNING:transformers.training_args:Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "WARNING:transformers.training_args:Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "{\"loss\": 0.09981145222334362, \"learning_rate\": 4.811320754716982e-05, \"epoch\": 1.8744186046511628, \"step\": 100}\n",
            "{\"loss\": 0.02321981748561484, \"learning_rate\": 4.6226415094339625e-05, \"epoch\": 3.7627906976744185, \"step\": 200}\n",
            "{\"loss\": 0.007212238606253436, \"learning_rate\": 4.433962264150944e-05, \"epoch\": 5.651162790697675, \"step\": 300}\n",
            "{\"loss\": 0.002417378737398579, \"learning_rate\": 4.245283018867925e-05, \"epoch\": 7.53953488372093, \"step\": 400}\n",
            "{\"loss\": 0.0017307399103767196, \"learning_rate\": 4.0566037735849064e-05, \"epoch\": 9.427906976744186, \"step\": 500}\n",
            "{\"loss\": 0.002722826146665511, \"learning_rate\": 3.867924528301887e-05, \"epoch\": 11.316279069767441, \"step\": 600}\n",
            "{\"loss\": 0.005149584150399491, \"learning_rate\": 3.679245283018868e-05, \"epoch\": 13.204651162790698, \"step\": 700}\n",
            "{\"loss\": 0.0011055616351605835, \"learning_rate\": 3.490566037735849e-05, \"epoch\": 15.093023255813954, \"step\": 800}\n",
            "{\"loss\": 0.001260166363214914, \"learning_rate\": 3.30188679245283e-05, \"epoch\": 16.967441860465115, \"step\": 900}\n",
            "{\"loss\": 0.0007972519813900014, \"learning_rate\": 3.113207547169811e-05, \"epoch\": 18.855813953488372, \"step\": 1000}\n",
            "{\"loss\": 0.0016674244849446218, \"learning_rate\": 2.9245283018867926e-05, \"epoch\": 20.74418604651163, \"step\": 1100}\n",
            "{\"loss\": 0.0014398315356753243, \"learning_rate\": 2.7358490566037738e-05, \"epoch\": 22.632558139534883, \"step\": 1200}\n",
            "{\"loss\": 0.0029017164134222994, \"learning_rate\": 2.547169811320755e-05, \"epoch\": 24.52093023255814, \"step\": 1300}\n",
            "{\"loss\": 0.0010839741860489127, \"learning_rate\": 2.358490566037736e-05, \"epoch\": 26.409302325581397, \"step\": 1400}\n",
            "{\"loss\": 0.0008799447792521242, \"learning_rate\": 2.1698113207547172e-05, \"epoch\": 28.29767441860465, \"step\": 1500}\n",
            "{\"loss\": 0.0011704891203335776, \"learning_rate\": 1.9811320754716984e-05, \"epoch\": 30.186046511627907, \"step\": 1600}\n",
            "{\"loss\": 0.00031741249654414715, \"learning_rate\": 1.7924528301886792e-05, \"epoch\": 32.074418604651164, \"step\": 1700}\n",
            "{\"loss\": 1.1132102380422282e-05, \"learning_rate\": 1.6037735849056604e-05, \"epoch\": 33.948837209302326, \"step\": 1800}\n",
            "{\"loss\": 3.0216013087578374e-05, \"learning_rate\": 1.4150943396226415e-05, \"epoch\": 35.83720930232558, \"step\": 1900}\n",
            "{\"loss\": 1.224922716287935e-05, \"learning_rate\": 1.2264150943396227e-05, \"epoch\": 37.72558139534884, \"step\": 2000}\n",
            "{\"loss\": 6.4144115428454995e-06, \"learning_rate\": 1.0377358490566038e-05, \"epoch\": 39.61395348837209, \"step\": 2100}\n",
            "{\"loss\": 4.7890591178774144e-05, \"learning_rate\": 8.49056603773585e-06, \"epoch\": 41.50232558139535, \"step\": 2200}\n",
            "{\"loss\": 0.00021513645642098566, \"learning_rate\": 6.60377358490566e-06, \"epoch\": 43.390697674418604, \"step\": 2300}\n",
            "{\"loss\": 4.0770877012441535e-06, \"learning_rate\": 4.716981132075472e-06, \"epoch\": 45.27906976744186, \"step\": 2400}\n",
            "{\"loss\": 6.5363322187650395e-06, \"learning_rate\": 2.830188679245283e-06, \"epoch\": 47.16744186046512, \"step\": 2500}\n",
            "{\"loss\": 1.0665643459084606e-05, \"learning_rate\": 9.433962264150943e-07, \"epoch\": 49.055813953488375, \"step\": 2600}\n",
            "WARNING:transformers.training_args:Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "WARNING:__main__:Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "evaluating uniform ptntime:\n",
            "Overall Acc: 0.7829566854990584\n",
            "Start Acc: 0.8082120582120582\n",
            "End Acc: 0.7620481927710844\n",
            "Story Acc: 0.27485380116959063\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **SYMTIME**"
      ],
      "metadata": {
        "id": "WCouBtxs4rVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_and_eval(**config_dict['uniform']['symtime'], per_gpu_train_batch_size=4,\n",
        "                    per_device_train_batch_size=4, gradient_accumulation_steps=2,\n",
        "                    per_device_eval_batch_size=4, per_gpu_eval_batch_size=4)\n",
        "print('evaluating uniform symtime:')\n",
        "evaluate_symbolic(config_dict['uniform']['symtime']['eval_data_file'], config_dict['uniform']['symtime']['output_dir'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_pcn72U4rzc",
        "outputId": "6b60d3d9-769a-48cd-d070-d921170cb576"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA SIZE: \n",
            "860\n",
            "DATA SIZE: \n",
            "4248\n",
            "WARNING:transformers.trainer:You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.\n",
            "WARNING:transformers.training_args:Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "WARNING:transformers.training_args:Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "{\"loss\": 0.5308013459974245, \"learning_rate\": 4.9065420560747664e-05, \"epoch\": 0.9302325581395349, \"step\": 100}\n",
            "{\"loss\": 0.3894613097977617, \"learning_rate\": 4.813084112149533e-05, \"epoch\": 1.8651162790697673, \"step\": 200}\n",
            "{\"loss\": 0.3172012095361461, \"learning_rate\": 4.719626168224299e-05, \"epoch\": 2.8, \"step\": 300}\n",
            "{\"loss\": 0.3344975383797083, \"learning_rate\": 4.6261682242990654e-05, \"epoch\": 3.734883720930233, \"step\": 400}\n",
            "{\"loss\": 0.2997360674394497, \"learning_rate\": 4.532710280373832e-05, \"epoch\": 4.669767441860465, \"step\": 500}\n",
            "{\"loss\": 0.2866877269521768, \"learning_rate\": 4.4392523364485984e-05, \"epoch\": 5.604651162790698, \"step\": 600}\n",
            "{\"loss\": 0.31086548764135613, \"learning_rate\": 4.3457943925233645e-05, \"epoch\": 6.53953488372093, \"step\": 700}\n",
            "{\"loss\": 0.2585303731220617, \"learning_rate\": 4.2523364485981306e-05, \"epoch\": 7.474418604651163, \"step\": 800}\n",
            "{\"loss\": 0.24505013398113534, \"learning_rate\": 4.1588785046728974e-05, \"epoch\": 8.409302325581395, \"step\": 900}\n",
            "{\"loss\": 0.29668482737695967, \"learning_rate\": 4.0654205607476636e-05, \"epoch\": 9.344186046511627, \"step\": 1000}\n",
            "{\"loss\": 0.25474745195218135, \"learning_rate\": 3.97196261682243e-05, \"epoch\": 10.279069767441861, \"step\": 1100}\n",
            "{\"loss\": 0.2593190927492992, \"learning_rate\": 3.8785046728971965e-05, \"epoch\": 11.213953488372093, \"step\": 1200}\n",
            "{\"loss\": 0.2532740207216534, \"learning_rate\": 3.7850467289719626e-05, \"epoch\": 12.148837209302325, \"step\": 1300}\n",
            "{\"loss\": 0.25195702520812346, \"learning_rate\": 3.691588785046729e-05, \"epoch\": 13.083720930232559, \"step\": 1400}\n",
            "{\"loss\": 0.2554013092368922, \"learning_rate\": 3.5981308411214956e-05, \"epoch\": 14.018604651162791, \"step\": 1500}\n",
            "{\"loss\": 0.28131101192789004, \"learning_rate\": 3.504672897196262e-05, \"epoch\": 14.948837209302326, \"step\": 1600}\n",
            "{\"loss\": 0.23737426822768215, \"learning_rate\": 3.411214953271028e-05, \"epoch\": 15.883720930232558, \"step\": 1700}\n",
            "{\"loss\": 0.2663879688958292, \"learning_rate\": 3.3177570093457946e-05, \"epoch\": 16.81860465116279, \"step\": 1800}\n",
            "{\"loss\": 0.23775970132566612, \"learning_rate\": 3.224299065420561e-05, \"epoch\": 17.753488372093024, \"step\": 1900}\n",
            "{\"loss\": 0.2548200756714175, \"learning_rate\": 3.130841121495327e-05, \"epoch\": 18.688372093023254, \"step\": 2000}\n",
            "{\"loss\": 0.23646675825608782, \"learning_rate\": 3.0373831775700934e-05, \"epoch\": 19.623255813953488, \"step\": 2100}\n",
            "{\"loss\": 0.26209740524790165, \"learning_rate\": 2.9439252336448598e-05, \"epoch\": 20.558139534883722, \"step\": 2200}\n",
            "{\"loss\": 0.24934484206642651, \"learning_rate\": 2.850467289719626e-05, \"epoch\": 21.493023255813952, \"step\": 2300}\n",
            "{\"loss\": 0.24402369284123893, \"learning_rate\": 2.7570093457943924e-05, \"epoch\": 22.427906976744186, \"step\": 2400}\n",
            "{\"loss\": 0.24837011552972854, \"learning_rate\": 2.663551401869159e-05, \"epoch\": 23.36279069767442, \"step\": 2500}\n",
            "{\"loss\": 0.24712522923256303, \"learning_rate\": 2.570093457943925e-05, \"epoch\": 24.29767441860465, \"step\": 2600}\n",
            "{\"loss\": 0.2660035844175866, \"learning_rate\": 2.4766355140186918e-05, \"epoch\": 25.232558139534884, \"step\": 2700}\n",
            "{\"loss\": 0.2535439504440467, \"learning_rate\": 2.383177570093458e-05, \"epoch\": 26.167441860465118, \"step\": 2800}\n",
            "{\"loss\": 0.24925327620855, \"learning_rate\": 2.2897196261682244e-05, \"epoch\": 27.10232558139535, \"step\": 2900}\n",
            "{\"loss\": 0.23984838698083422, \"learning_rate\": 2.196261682242991e-05, \"epoch\": 28.037209302325582, \"step\": 3000}\n",
            "{\"loss\": 0.25847319773804317, \"learning_rate\": 2.102803738317757e-05, \"epoch\": 28.967441860465115, \"step\": 3100}\n",
            "{\"loss\": 0.2569513232219754, \"learning_rate\": 2.0093457943925235e-05, \"epoch\": 29.90232558139535, \"step\": 3200}\n",
            "{\"loss\": 0.2555680581851755, \"learning_rate\": 1.9158878504672896e-05, \"epoch\": 30.837209302325583, \"step\": 3300}\n",
            "{\"loss\": 0.24161695415472537, \"learning_rate\": 1.822429906542056e-05, \"epoch\": 31.772093023255813, \"step\": 3400}\n",
            "{\"loss\": 0.24988580933922436, \"learning_rate\": 1.7289719626168225e-05, \"epoch\": 32.70697674418604, \"step\": 3500}\n",
            "{\"loss\": 0.23409088853127627, \"learning_rate\": 1.6355140186915887e-05, \"epoch\": 33.64186046511628, \"step\": 3600}\n",
            "{\"loss\": 0.242871417481972, \"learning_rate\": 1.542056074766355e-05, \"epoch\": 34.57674418604651, \"step\": 3700}\n",
            "{\"loss\": 0.25657356671664044, \"learning_rate\": 1.4485981308411214e-05, \"epoch\": 35.51162790697674, \"step\": 3800}\n",
            "{\"loss\": 0.20773587912454786, \"learning_rate\": 1.3551401869158877e-05, \"epoch\": 36.44651162790698, \"step\": 3900}\n",
            "{\"loss\": 0.24600111586792764, \"learning_rate\": 1.2616822429906542e-05, \"epoch\": 37.38139534883721, \"step\": 4000}\n",
            "{\"loss\": 0.2254407391678842, \"learning_rate\": 1.1682242990654207e-05, \"epoch\": 38.31627906976744, \"step\": 4100}\n",
            "{\"loss\": 0.24220619929403028, \"learning_rate\": 1.074766355140187e-05, \"epoch\": 39.25116279069768, \"step\": 4200}\n",
            "{\"loss\": 0.23928734956311473, \"learning_rate\": 9.813084112149533e-06, \"epoch\": 40.18604651162791, \"step\": 4300}\n",
            "{\"loss\": 0.2514267630201402, \"learning_rate\": 8.878504672897196e-06, \"epoch\": 41.12093023255814, \"step\": 4400}\n",
            "{\"loss\": 0.2429216415290375, \"learning_rate\": 7.94392523364486e-06, \"epoch\": 42.055813953488375, \"step\": 4500}\n",
            "{\"loss\": 0.24801086625092467, \"learning_rate\": 7.009345794392523e-06, \"epoch\": 42.986046511627904, \"step\": 4600}\n",
            "{\"loss\": 0.24063206923073266, \"learning_rate\": 6.074766355140187e-06, \"epoch\": 43.92093023255814, \"step\": 4700}\n",
            "{\"loss\": 0.23564091548940724, \"learning_rate\": 5.14018691588785e-06, \"epoch\": 44.85581395348837, \"step\": 4800}\n",
            "{\"loss\": 0.2555193062582657, \"learning_rate\": 4.205607476635514e-06, \"epoch\": 45.7906976744186, \"step\": 4900}\n",
            "{\"loss\": 0.22249452398483754, \"learning_rate\": 3.2710280373831774e-06, \"epoch\": 46.72558139534884, \"step\": 5000}\n",
            "{\"loss\": 0.2505224841370432, \"learning_rate\": 2.3364485981308413e-06, \"epoch\": 47.66046511627907, \"step\": 5100}\n",
            "{\"loss\": 0.24278252184177063, \"learning_rate\": 1.4018691588785047e-06, \"epoch\": 48.5953488372093, \"step\": 5200}\n",
            "{\"loss\": 0.2451427662166566, \"learning_rate\": 4.672897196261682e-07, \"epoch\": 49.53023255813954, \"step\": 5300}\n",
            " \n",
            "evaluating uniform symtime:\n",
            "Start Acc: 0.8269230769230769\n",
            "End Acc: 0.7822719449225474\n",
            "Overall Acc: 0.8101101749837978\n",
            "Story Acc: 0.3362573099415205\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **SYMTIME-Zero Shot**"
      ],
      "metadata": {
        "id": "3ObksQtb5D6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_and_eval(**config_dict['uniform']['symtime'], per_gpu_train_batch_size=4,\n",
        "                    per_device_train_batch_size=4, gradient_accumulation_steps=2,\n",
        "                    per_device_eval_batch_size=4, per_gpu_eval_batch_size=4,\n",
        "                    do_train=False, eval_model_path=SYMTIME_PRETRAINED_MODEL_START)\n",
        "print('evaluating uniform symtime zero-shot:')\n",
        "evaluate_symbolic(config_dict['uniform']['symtime']['eval_data_file'], config_dict['uniform']['symtime']['output_dir'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qzfObxh5ENd",
        "outputId": "b6673724-c29f-4ad7-f175-059a3f728993"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA SIZE: \n",
            "4248\n",
            "WARNING:transformers.trainer:You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.\n",
            "WARNING:transformers.training_args:Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "evaluating uniform symtime zero-shot:\n",
            "Start Acc: 0.7697505197505198\n",
            "End Acc: 0.7306368330464716\n",
            "Overall Acc: 0.7550226830848995\n",
            "Story Acc: 0.21637426900584794\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MATRES"
      ],
      "metadata": {
        "id": "fORQ9NWL9zJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Baseline (T5)**"
      ],
      "metadata": {
        "id": "7wgpwpbb5Ory"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_and_eval(**config_dict['matres']['base_model'])\n",
        "print('evaluating matres base model:')\n",
        "evaluate_tracie_style(config_dict['matres']['base_model']['eval_data_file'],\n",
        "                      config_dict['matres']['base_model']['output_dir'], end=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv-RcHOb5Og1",
        "outputId": "35cc5d32-ffc2-47bc-e7eb-c7f2a8f3bd2c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "-1\n",
            "DATA SIZE: \n",
            "1249\n",
            "DATA SIZE: \n",
            "1322\n",
            "WARNING:transformers.trainer:You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.\n",
            "WARNING:transformers.training_args:Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "WARNING:transformers.training_args:Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "{\"loss\": 0.7645370342116803, \"learning_rate\": 4.871794871794872e-05, \"epoch\": 1.281150159744409, \"step\": 100}\n",
            "{\"loss\": 0.2548688959516585, \"learning_rate\": 4.7435897435897435e-05, \"epoch\": 2.562300319488818, \"step\": 200}\n",
            "{\"loss\": 0.25069136084988713, \"learning_rate\": 4.615384615384616e-05, \"epoch\": 3.8434504792332267, \"step\": 300}\n",
            "{\"loss\": 0.2370670727081597, \"learning_rate\": 4.4871794871794874e-05, \"epoch\": 5.127795527156549, \"step\": 400}\n",
            "{\"loss\": 0.2133384732529521, \"learning_rate\": 4.358974358974359e-05, \"epoch\": 6.4089456869009584, \"step\": 500}\n",
            "{\"loss\": 0.18363455828744918, \"learning_rate\": 4.230769230769231e-05, \"epoch\": 7.6900958466453675, \"step\": 600}\n",
            "{\"loss\": 0.1416981514589861, \"learning_rate\": 4.1025641025641023e-05, \"epoch\": 8.971246006389777, \"step\": 700}\n",
            "{\"loss\": 0.10124965930051985, \"learning_rate\": 3.974358974358974e-05, \"epoch\": 10.255591054313099, \"step\": 800}\n",
            "{\"loss\": 0.07524656506167958, \"learning_rate\": 3.846153846153846e-05, \"epoch\": 11.536741214057509, \"step\": 900}\n",
            "{\"loss\": 0.05700894778030488, \"learning_rate\": 3.717948717948718e-05, \"epoch\": 12.817891373801917, \"step\": 1000}\n",
            "{\"loss\": 0.04029157878188471, \"learning_rate\": 3.58974358974359e-05, \"epoch\": 14.10223642172524, \"step\": 1100}\n",
            "{\"loss\": 0.031456004829782384, \"learning_rate\": 3.461538461538462e-05, \"epoch\": 15.383386581469649, \"step\": 1200}\n",
            "{\"loss\": 0.023178270110767018, \"learning_rate\": 3.3333333333333335e-05, \"epoch\": 16.664536741214057, \"step\": 1300}\n",
            "{\"loss\": 0.01711563418952238, \"learning_rate\": 3.205128205128206e-05, \"epoch\": 17.945686900958467, \"step\": 1400}\n",
            "{\"loss\": 0.012160859655150489, \"learning_rate\": 3.0769230769230774e-05, \"epoch\": 19.230031948881788, \"step\": 1500}\n",
            "{\"loss\": 0.008974643268524289, \"learning_rate\": 2.948717948717949e-05, \"epoch\": 20.511182108626198, \"step\": 1600}\n",
            "{\"loss\": 0.007481163275511165, \"learning_rate\": 2.8205128205128207e-05, \"epoch\": 21.792332268370608, \"step\": 1700}\n",
            "{\"loss\": 0.008896350283994253, \"learning_rate\": 2.6923076923076923e-05, \"epoch\": 23.076677316293928, \"step\": 1800}\n",
            "{\"loss\": 0.0083928427373192, \"learning_rate\": 2.564102564102564e-05, \"epoch\": 24.357827476038338, \"step\": 1900}\n",
            "{\"loss\": 0.0049325212935013955, \"learning_rate\": 2.435897435897436e-05, \"epoch\": 25.638977635782748, \"step\": 2000}\n",
            "{\"loss\": 0.009791226138167417, \"learning_rate\": 2.307692307692308e-05, \"epoch\": 26.920127795527158, \"step\": 2100}\n",
            "{\"loss\": 0.004204390294738403, \"learning_rate\": 2.1794871794871795e-05, \"epoch\": 28.20447284345048, \"step\": 2200}\n",
            "{\"loss\": 0.0027645998663857084, \"learning_rate\": 2.0512820512820512e-05, \"epoch\": 29.48562300319489, \"step\": 2300}\n",
            "{\"loss\": 0.003794125734169427, \"learning_rate\": 1.923076923076923e-05, \"epoch\": 30.766773162939298, \"step\": 2400}\n",
            "{\"loss\": 0.0013579306216561803, \"learning_rate\": 1.794871794871795e-05, \"epoch\": 32.05111821086262, \"step\": 2500}\n",
            "{\"loss\": 0.0005544495176494024, \"learning_rate\": 1.6666666666666667e-05, \"epoch\": 33.33226837060703, \"step\": 2600}\n",
            "{\"loss\": 0.003538581657038549, \"learning_rate\": 1.5384615384615387e-05, \"epoch\": 34.61341853035144, \"step\": 2700}\n",
            "{\"loss\": 0.00033246073276615106, \"learning_rate\": 1.4102564102564104e-05, \"epoch\": 35.894568690095845, \"step\": 2800}\n",
            "{\"loss\": 0.0006425155973570895, \"learning_rate\": 1.282051282051282e-05, \"epoch\": 37.17891373801917, \"step\": 2900}\n",
            "{\"loss\": 0.0009452574622483212, \"learning_rate\": 1.153846153846154e-05, \"epoch\": 38.460063897763575, \"step\": 3000}\n",
            "{\"loss\": 0.0003287113676603326, \"learning_rate\": 1.0256410256410256e-05, \"epoch\": 39.74121405750799, \"step\": 3100}\n",
            "{\"loss\": 0.0022428860518812145, \"learning_rate\": 8.974358974358976e-06, \"epoch\": 41.02555910543131, \"step\": 3200}\n",
            "{\"loss\": 0.00198646559232742, \"learning_rate\": 7.692307692307694e-06, \"epoch\": 42.30670926517572, \"step\": 3300}\n",
            "{\"loss\": 0.0023979915808132546, \"learning_rate\": 6.41025641025641e-06, \"epoch\": 43.587859424920126, \"step\": 3400}\n",
            "{\"loss\": 0.0005192066980890787, \"learning_rate\": 5.128205128205128e-06, \"epoch\": 44.86900958466454, \"step\": 3500}\n",
            "{\"loss\": 0.00020982460926404655, \"learning_rate\": 3.846153846153847e-06, \"epoch\": 46.153354632587856, \"step\": 3600}\n",
            "{\"loss\": 4.8545208740620184e-05, \"learning_rate\": 2.564102564102564e-06, \"epoch\": 47.43450479233227, \"step\": 3700}\n",
            "{\"loss\": 3.646313604548368e-05, \"learning_rate\": 1.282051282051282e-06, \"epoch\": 48.715654952076676, \"step\": 3800}\n",
            "{\"loss\": 2.097321771884708e-05, \"learning_rate\": 0.0, \"epoch\": 49.99680511182109, \"step\": 3900}\n",
            "WARNING:transformers.training_args:Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "evaluating matres base model:\n",
            "Overall Acc: 0.7821482602118003\n",
            "Start Acc: 0.7821482602118003\n",
            "Story Acc: 0.6742424242424242\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **PTNTIME**"
      ],
      "metadata": {
        "id": "iedsZnKn5Zs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_and_eval(**config_dict['matres']['ptntime'])\n",
        "print('evaluating matres symtime:')\n",
        "evaluate_tracie_style(config_dict['matres']['ptntime']['eval_data_file'],\n",
        "                  config_dict['matres']['ptntime']['output_dir'], end=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PN7g1j75aFD",
        "outputId": "3ca4a051-5d96-4172-a21e-6cbd920d6d62"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "-1\n",
            "DATA SIZE: \n",
            "1249\n",
            "DATA SIZE: \n",
            "1322\n",
            "WARNING:transformers.trainer:You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.\n",
            "WARNING:transformers.training_args:Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "WARNING:transformers.training_args:Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "{\"loss\": 0.13706411827210105, \"learning_rate\": 4.871794871794872e-05, \"epoch\": 1.281150159744409, \"step\": 100}\n",
            "{\"loss\": 0.046504235440061166, \"learning_rate\": 4.7435897435897435e-05, \"epoch\": 2.562300319488818, \"step\": 200}\n",
            "{\"loss\": 0.014811778514387015, \"learning_rate\": 4.615384615384616e-05, \"epoch\": 3.8434504792332267, \"step\": 300}\n",
            "{\"loss\": 0.01162463910420854, \"learning_rate\": 4.4871794871794874e-05, \"epoch\": 5.127795527156549, \"step\": 400}\n",
            "{\"loss\": 0.0059101992199632036, \"learning_rate\": 4.358974358974359e-05, \"epoch\": 6.4089456869009584, \"step\": 500}\n",
            "{\"loss\": 0.008549527571371698, \"learning_rate\": 4.230769230769231e-05, \"epoch\": 7.6900958466453675, \"step\": 600}\n",
            "{\"loss\": 0.006868508261195672, \"learning_rate\": 4.1025641025641023e-05, \"epoch\": 8.971246006389777, \"step\": 700}\n",
            "{\"loss\": 0.001814581974083964, \"learning_rate\": 3.974358974358974e-05, \"epoch\": 10.255591054313099, \"step\": 800}\n",
            "{\"loss\": 9.451006486472125e-05, \"learning_rate\": 3.846153846153846e-05, \"epoch\": 11.536741214057509, \"step\": 900}\n",
            "{\"loss\": 0.003324816712873222, \"learning_rate\": 3.717948717948718e-05, \"epoch\": 12.817891373801917, \"step\": 1000}\n",
            "{\"loss\": 0.0005191099086669481, \"learning_rate\": 3.58974358974359e-05, \"epoch\": 14.10223642172524, \"step\": 1100}\n",
            "{\"loss\": 0.0014358978119057041, \"learning_rate\": 3.461538461538462e-05, \"epoch\": 15.383386581469649, \"step\": 1200}\n",
            "{\"loss\": 0.004584185560394864, \"learning_rate\": 3.3333333333333335e-05, \"epoch\": 16.664536741214057, \"step\": 1300}\n",
            "{\"loss\": 0.0017434138780724594, \"learning_rate\": 3.205128205128206e-05, \"epoch\": 17.945686900958467, \"step\": 1400}\n",
            "{\"loss\": 0.000750624211631532, \"learning_rate\": 3.0769230769230774e-05, \"epoch\": 19.230031948881788, \"step\": 1500}\n",
            "{\"loss\": 0.005162437036751654, \"learning_rate\": 2.948717948717949e-05, \"epoch\": 20.511182108626198, \"step\": 1600}\n",
            "{\"loss\": 0.0018653447596152972, \"learning_rate\": 2.8205128205128207e-05, \"epoch\": 21.792332268370608, \"step\": 1700}\n",
            "{\"loss\": 0.0029044417313206948, \"learning_rate\": 2.6923076923076923e-05, \"epoch\": 23.076677316293928, \"step\": 1800}\n",
            "{\"loss\": 0.0016748748717354545, \"learning_rate\": 2.564102564102564e-05, \"epoch\": 24.357827476038338, \"step\": 1900}\n",
            "{\"loss\": 0.001583228783213926, \"learning_rate\": 2.435897435897436e-05, \"epoch\": 25.638977635782748, \"step\": 2000}\n",
            "{\"loss\": 0.0029308466584858904, \"learning_rate\": 2.307692307692308e-05, \"epoch\": 26.920127795527158, \"step\": 2100}\n",
            "{\"loss\": 0.00037937330414791856, \"learning_rate\": 2.1794871794871795e-05, \"epoch\": 28.20447284345048, \"step\": 2200}\n",
            "{\"loss\": 0.0001110747487976127, \"learning_rate\": 2.0512820512820512e-05, \"epoch\": 29.48562300319489, \"step\": 2300}\n",
            "{\"loss\": 2.3091161289876538e-05, \"learning_rate\": 1.923076923076923e-05, \"epoch\": 30.766773162939298, \"step\": 2400}\n",
            "{\"loss\": 7.928007592070685e-05, \"learning_rate\": 1.794871794871795e-05, \"epoch\": 32.05111821086262, \"step\": 2500}\n",
            "{\"loss\": 0.0008077618309216561, \"learning_rate\": 1.6666666666666667e-05, \"epoch\": 33.33226837060703, \"step\": 2600}\n",
            "{\"loss\": 2.0420727889884916e-05, \"learning_rate\": 1.5384615384615387e-05, \"epoch\": 34.61341853035144, \"step\": 2700}\n",
            "{\"loss\": 6.012885492623354e-06, \"learning_rate\": 1.4102564102564104e-05, \"epoch\": 35.894568690095845, \"step\": 2800}\n",
            "{\"loss\": 0.0005068959453945965, \"learning_rate\": 1.282051282051282e-05, \"epoch\": 37.17891373801917, \"step\": 2900}\n",
            "{\"loss\": 1.5306653396898184e-05, \"learning_rate\": 1.153846153846154e-05, \"epoch\": 38.460063897763575, \"step\": 3000}\n",
            "{\"loss\": 2.711259128943766e-05, \"learning_rate\": 1.0256410256410256e-05, \"epoch\": 39.74121405750799, \"step\": 3100}\n",
            "{\"loss\": 5.86675713346807e-06, \"learning_rate\": 8.974358974358976e-06, \"epoch\": 41.02555910543131, \"step\": 3200}\n",
            "{\"loss\": 0.0018721241272584522, \"learning_rate\": 7.692307692307694e-06, \"epoch\": 42.30670926517572, \"step\": 3300}\n",
            "{\"loss\": 1.1124774381983115e-05, \"learning_rate\": 6.41025641025641e-06, \"epoch\": 43.587859424920126, \"step\": 3400}\n",
            "{\"loss\": 1.4848301665253416e-05, \"learning_rate\": 5.128205128205128e-06, \"epoch\": 44.86900958466454, \"step\": 3500}\n",
            "{\"loss\": 6.498651378734621e-06, \"learning_rate\": 3.846153846153847e-06, \"epoch\": 46.153354632587856, \"step\": 3600}\n",
            "{\"loss\": 2.0078888923791284e-06, \"learning_rate\": 2.564102564102564e-06, \"epoch\": 47.43450479233227, \"step\": 3700}\n",
            "{\"loss\": 9.140669714653882e-05, \"learning_rate\": 1.282051282051282e-06, \"epoch\": 48.715654952076676, \"step\": 3800}\n",
            "{\"loss\": 1.7270213474773755e-06, \"learning_rate\": 0.0, \"epoch\": 49.99680511182109, \"step\": 3900}\n",
            "WARNING:transformers.training_args:Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "evaluating matres symtime:\n",
            "\n",
            "Overall Acc: 0.869894099848714\n",
            "Start Acc: 0.869894099848714\n",
            "Story Acc: 0.85\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "YpHAWcYr-LxI",
        "R9N4x-Bd-Nsh",
        "qI1B2LR1-T7R",
        "PHdKco7P6yZd",
        "kIiBZjzWv9mp",
        "GAwbz84oxRJR",
        "CsmfFREK7hz6",
        "QmwVoz0756lE",
        "MFh9oJQY50mx",
        "DMb--NLv5zyZ",
        "6AHy3PCn5zLY",
        "uhyMyYK2fkhV",
        "xmUh0knL5yYH",
        "aAIXT-1H87On",
        "Buyie-zn3b8p",
        "G2ZOi94e33Da",
        "BSD2GNPq45QM",
        "JraFt-Jl4JLl",
        "O7AukwIU4dmn",
        "WCouBtxs4rVP",
        "7wgpwpbb5Ory",
        "iedsZnKn5Zs1"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}